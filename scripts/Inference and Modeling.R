### DATA SCIENCE: INFERENCE AND MODELING

# In this course, you will learn:

# The concepts necessary to define estimates and margins of errorr of populations, parameters,
# estimates, and standard errors in order to make predictions about data.

# How to use models to aggregate data from different sources.

# The very basics of Bayesian statistics and predictive modeling.

## INTRODUCTION

# Introduction to inference

# In this course, you will learn:

# statistical inference, the process of deducing characteristics of a population using
# data from a random sample.

# the statistical concepts necessary to define estimates and margins of errors.

# how to forecast future results and estimate the precision of our forecast.

# how to calculate and interpret confidence intervals and p-values.

# Information gathered from a small random sample can be used to infer characteristics
# of the entire population.

# Opinion polls are useful when asking everyone in the population is impossible.

# A common use for opinion polls is determining voter preferences in political elections
# for the purposes of forecasting election results.

# The spread of a poll is the estimated difference between the support for two candidates
# or options.

## SECTION 1. PARAMETERS AND ESTIMATES

# Section 1 introduces you to parameters and estimates

# You will be able to:

# understand how to use a sampling model to perform a poll
# explain the terms population, parameter, and sample as they relate to statistical inference
# use a sample to estimate the population proportion from the sample average
# calculate the expected value and standard error of the sample average

# SAMPLING MODEL PARAMETERS AND ESTIMATES

# The task of statistical inference is to estimate an unknown population parameter using
# observed data from a sample.

# In a sampling model, the collection of elements in the urn is called the population.

# A parameter is a number that summarizes data for an entire population.

# A sample is observed data from a subset of the population.

# An estimate is a summary of the observed data about a parameter that we believe is
# informative. It is a data-driven guess of the population parameter.

# We want to predict the proportion of the blue beads in the urn, the parameter p. The
# proportion of red beads in the urn is 1 - p and the spread is 2p - 1.

# The sample proportion is a random variable. Sampling gives random results drawn from
# the population distribution.

# Function for taking a random draw from a specific urn

# The dslabs package includes a function for taking a random draw of size n from the
# urn described in the video:
library(tidyverse)
library(dslabs)
take_poll(25) # draw 25 beads

# THE SAMPLE AVERAGE

# Many common data science tasks can be framed as estimating a parameter from a sample.

# We illustrate statistical inference by walking through the process to estimate p. From
# the estimate of p, we can easily calculate an estimate of the spread, 2p - 1.

# Consider the random variable X that is 1 if a blue bead is chosen and 0 if a red bead
# is chosen. The proportion of blue beads in N draws is the average of the draws X1,...,XN.

# X[-] is the sample average. In statistics, a bar on top of a symbol denotes the average.
# X[-] is a random variable because it is the average of random draws - each time we take
# a sample, X[-] is different: X[-] = X1+X2+...+XN / N

# The number of blue beads drawn in N draws NX[-], is N times the proportion of values in
# the urn. However, we do not know the true proportion: we are trying to estimate this
# parameter p.

# POLLING VERSUS FORECASTING

# A poll taken in advance of an election estimates p for that moment, not for election day.

# In order to predict election results, forecasters try to use early estimates of p to
# predict p on election day. We discuss some approaches in later sections.

# PROPERTIES OF OUR ESTIMATE

# When interpreting values of X[-], it is important to remember that X[-] is a random
# variable with an expected value and standard error that represents the sample proportion
# of positive events.

# The expected value of X[-] is the parameter of interest p. This follows from the fact
# that X[-] is the sum of independent draws of a random variable times a constant 1/N.
# E(X[-]) = p

# As the number of draws N increases, the standard error of our estimate X[-] decreases.
# The standard error of the average of X[-] over N draws is:
# SE(X[-]) = sqrt(p*(1-p)/N)

# In theory, we can get more accurate estimates of p by increasing N. In practice, there
# are limits on the size of N due to costs, as well as other factors we discuss later.

# We can also use other random variable equations to determine the expected value of the
# sum of draws E(S) and standard error of the sum of draws SE(S).
# E(S) = Np
# SE(S) = sqrt(Np(1-p))

# ASSESSMENT 1.1. PARAMETERS AND ESTIMATES

# Exercise 1. Polling - expected value of S

# Suppose you poll a population in which a proportion p of voters are Democrats and
# 1 - p are Republicans. Your sample size is N = 25. Consider the random variable S,
# which is the total number of Democrats in your sample.

# What is the expected value of this random variable S?

# E(S) = 25(1 - p)

# E(S) = 25p # Correct

# E(S) = sqrt(25p(1-p))

# E(S) = p

# Exercise 2. Polling - standard error of S

# Again, consider the random variable S, which is the total number of Democrats in
# your sample of 25 voters. The variable p describes the proportion of Democrats in
# the sample, whereas 1 - p describes the proportion of Republicans.

# What is the standard error of S?

# SE(S) = 25p(1 - p)

# SE(S) = sqrt(25p)

# SE(S) = 25(1 - p)

# SE(S) = sqrt(25p(1 - p)) # Correct

# Exercise 3. Polling - expected value of X-bar

# Consider the random variable S/N, which is equivalent to the sample average that
# we have been denoting as X-bar. The variable N represents the sample size and p is
# the proportion of Democrats in the population.

# What is the expected value of X-bar?

# E(X-bar) = p # Correct

# E(X-bar) = Np

# E(X-bar) = N(1 - p)

# E(X-bar) = 1 - p

# Exercise 4. Polling - standard error of X-bar

# What is the standard error of the sample average, X-bar?

# The variable N represents the sample size and p is the proportion of Democrats in
# the population.

# SE(X-bar) = sqrt(Np(1 - p))

# SE(X-bar) = sqrt(p(1 - p)/N) # Correct

# SE(X-bar) = sqrt(p(1 - p))

# SE(X-bar) = sqrt(N)

# Exercise 5. se versus p

# Write a line of code that calculates the standard error se of a sample average when
# you poll 25 people in the population. Generate a sequence of 100 proportions of
# Democrats p that vary from 0 (no Democrats) to 1 (all Democrats).

# Plot se versus p for the 100 different proportions.

# Use the seq function to generate a vector of 100 values of p that range from 0 to 1.

# Use the sqrt function to generate a vector of standard errors for all values of p.

# Use the plot function to generate a plot with p on the x-axis and se on the y-axis.

# `N` represents the number of people polled
N <- 25

# Create a variable `p` that contains 100 proportions ranging from 0 to 1 using the `seq`
# function

p <- seq(0, 1, length = 100)

# Create a variable `se` that contains the standard error of each sample average

se <- sqrt(p * (1 - p) / N)

# Plot `p` on the x-axis and `se` on the y-axis

plot(p, se)

# Exercise 6. Multiple plots of se versus p

# Using the same code as in the previous exercise, create a for-loop that generates
# three plots of p versus se when the sample sizes equal N = 25, N = 100, and N = 1000.

# Your for-loop should contain two lines of code to be repeated for three different
# values of N.

# The first line within the for-loop should use the sqrt function to generate a vector
# of standard errors se for all values o p.

# The second line within the for-loop should use the plot function to generate a plot
# with p on the x-axis and se on the y-axis.

# Use they ylim argument to keep the y-axis limits constant across all three plots. The
# lower limit should be equal to 0 and the upper limit should be equal to 0.1 (it can be
# shown that this value is the highest calculated standard error across all values of p
# and N).

# The vector `p` contains 100 proportions of Democrats ranging from 0 to 1 using the 
# `seq` function
p <- seq(0, 1, length = 100)

# The vector `sample_sizes` contains the three sample sizes
sample_sizes <- c(25, 100, 1000)

# Write a for-loop that calculates the standard error `se` for every value of `p` for
# each of the three sample sizes `N` in the vector `sample_sizes`. Plot the three graphs,
# using the `ylim` argument to standardize the y-axis across all three plots.

for(i in sample_sizes){
  se <- sqrt(p * (1 - p) / i)
  plot(p, se, ylim = c(0, 0.1))
}

# Exercise 7. Expected value of d

# Our estimate for the difference in proportions of Democrats and Republicans is
# d = X-bar - (1 - X-bar).

# Which derivation correctly uses the rules we learned about sums of random variables
# and scaled random variables to derive the expected value of d?

# E[X-bar - (1 - X-bar)] = E[2X-bar - 1] = 2E[X-bar] - 1 = N(2p - 1) = Np - N(1 - p)

# E[X-bar - (1 - X-bar)] = E[X-bar - 1] = E[X-bar] - 1 = p - 1

# E[X-bar - (1 - X-bar)] = E[2X-bar - 1] = 2E[X-bar] - 1 = 2*sqrt(p(1 - p)) - 1 = p - (1 - p)

# E[X-bar - (1 - X-bar)] = E[2X-bar - 1] = 2E[X-bar] - 1 = 2p - 1 = p - (1 - p) # Correct

# Exercise 8. Standard error of d

# Our estimate for the difference in proportions of Democrats and Republicans is
# d = X-bar - (1 - X-bar).

# Which derivation correctly uses the rules we learned about sums of random variables and
# scaled random variables to derive the standard error of d?

# SE[X-bar - (1 - X-bar)] = SE[2X-bar - 1] = 2SE[X-bar] = 2*sqrt(p/N)

# SE[X-bar - (1 - X-bar)] = SE[2X-bar - 1] = 2SE[X-bar - 1] = 2*sqrt(p(1 - p)/N) - 1

# SE[X-bar - (1 - X-bar)] = SE[2X-bar - 1] = 2SE[X-bar] = 2*sqrt(p(1-p)/N) # Correct

# SE[X-bar - (1 - X-bar)] = SE[X-bar - 1] = SE[X-bar] = sqrt(p(1-p)/N)

# Exercise 9. Standard error of the spread

# Say the actual proportion of Democratic voters is p = 0.45. In this case, the
# Republican party is winning by a relatively large margin of d = -0.1, or a 10%
# margin of victory. What is the standard error of the spread 2X-bar - 1 in this case?

# Use the sqrt function to calculate the standard error of the spread 2X-bar - 1.

# `N` represents the number of people polled
N <- 25

# `p` represents the proportion of Democratic voters
p <- 0.45

# Calculate the standard error of the spread. Print this value to the console.

2 * sqrt(p * (1 - p) / N)

# Exercise 10. Sample size

# So far we have said that the difference between the proportion of Democratic
# voters and Republican voters in about 10% and that the standard error of this
# spread is about 10% and that the standard error of this spread is about 0.2
# when N = 25. Select the statement that explains why this sample size is sufficient
# or not.

# This sample size is sufficient because the expected value of our estimate 2X-bar - 1
# is d so our prediction will be right on.

# The sample size is too small because the standard error is larger than the spread. # Correct

# This sample size is sufficient because the standard error of about 0.2 is much smaller
# than the spread of 10%.

# Without knowing p, we have no way of knowing that increasing our sample size would
# actually improve our standard error.

## SECTION 2. THE CENTRAL LIMIT THEOREM IN PRACTICE

# You will be able to:

# use the Central Limit Theorem to calculate the probability that a sample estimate
# X-bar is close to the population proportion p.

# run a Monte Carlo simulation to corroborate theoretical results built using probability
# theory.

# estimate the spread based on estimates of X-bar and SE^(X-bar).

# uderstand why bias can mean that larger sample sizes aren't necessarily better.

# THE CENTRAL LIMIT THEOREM IN PRACTICE

# Because X-bar is the sum of random draws divided by a constant, the distribution of
# X-bar is approximately normal.

# We can convert X-bar to a standard normal random variable Z:
# Z = X-bar - E(X-bar) / SE(X-bar)

# The probability that X-bar is within .01 of the actual value p is:
# Pr(Z <= .01 / sqrt(p(1-p)/N)) - Pr(Z <= -.01 / sqrt(p(1-p)/N))

# The Central Limit Theorem (CLT) still works if X-bar is used in place of p. This is
# called a plug-in estimate. Hats (^) over values denotes estimates. Therefore:

# SE^(X-bar) = sqrt(X-bar(1 - X-bar)/N)

# Using the CLT, the probability that X-bar is within .01 of the actual value of p is:
# Pr(Z <= .01 / sqrt(X-bar(1 - X-bar)/N)) - Pr(Z <= -.01 / sqrt(X-bar(1 - X-bar)/N))

# Computing the probability of X-bar being within .01 of p

X_hat <- 0.48 # observed probability in the N = 25 sample
se <- sqrt(X_hat * (1 - X_hat) / 25)
pnorm(0.01/se) - pnorm(-0.01/se) # ~ 8% probability that p is within 1-point of X_hat

# MARGIN OF ERROR

# The margin of error is defined as 2 times the standard error of the estimate X-bar.

# There is about a 95% chance that X-bar will be within two standard errors of the
# actual parameter p.

# A MONTE CARLO SIMULATION FOR THE CLT

# We can run Monte Carlo simulations to compare with theoretical results assuming
# a value of p.

# In practice, p is unknown. We can corroborate theoretical results by running Monte
# Carlo simulations with one or several values of p.

# One practical choice for p when modeling is X-bar, the observed value of X^ in a sample.

# Monte Carlo simulation using a set value of p

p <- 0.45 # unknown p to estimate
N <- 1000

# Simulate one poll of size N and determine X_hat
x <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
x_hat <- mean(x)

# Simulate B polls of size N and determine the average x_hat
B <- 10000 # number of replicates
N <- 1000 # sample size per replicate
x_hat <- replicate(B, {
  x <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})

# Histogram and QQ-plot of Monte Carlo results

library(tidyverse)
library(gridExtra)
p1 <- data.frame(x_hat = x_hat) %>%
  ggplot(aes(x_hat)) +
  geom_histogram(binwidth = 0.005, color = "black")
p2 <- data.frame(x_hat = x_hat) %>%
  ggplot(aes(sample = x_hat)) +
  stat_qq(dparams = list(mean = mean(x_hat), sd = sd(x_hat))) +
  geom_abline() +
  ylab("X_hat") +
  xlab("Theoretical normal")
grid.arrange(p1, p2, nrow = 1)

# THE SPREAD

# The spread between two outcomes with probabilities p and 1 - p is 2p - 1

# The expected value of the spread is 2X-bar - 1

# The standard error of the spread is 2SE^(X-bar)

# The margin of error of the spread is 2 times the margin of error of X-bar

# BIAS: WHY NOT RUN A VERY LARGE POLL?

# An extremely large poll would theoretically be able to predict election results
# almost perfectly.

# These sample sizes are not practical. In addition to cost concerns, polling does
# not reach everyone in the population (eventual voters) with equal probability,
# and it also may include data from outside our population (people who will not
# end up voting).

# These systematic errors in polling are called bias. We will learn more about
# bias in the future.

# Plotting margin of error in an extremely large poll over a range of values of p
library(tidyverse)
N <- 100000
p <- seq(0.35, 0.65, length = 100)
se <- sapply(p, function(x) 2 * sqrt(x * (1 - x) / N))
data.frame(p = p, se = se) %>%
  ggplot(aes(p, se)) +
  geom_line()

# ASSESSMENT 2.1. INTRODUCTION TO INFERENCE

# Exercise 1. Sample average

# Write a function called take_sample that takes the proportion of Democrats p
# and the sample size N as arguments and returns the sample average of Democrats (1)
# and Republicans (0).

# Calculate the sample average if the proportion of Democrats equals 0.45 and the
# sample size is 100.

# Define a function called take_sample that takes p and N as arguments.

# Use the sample function as the first statement in your function to sample N
# elements from a vector of options where Democrats are assigned the value `1`
# and Republicans are assigned the value `0` in that order.

# Use the mean function as the second statement in your function to find the average
# value of the random sample.

# Write a function called `take_sample` that takes `p` and `N` as arguments and returns
# the average value of a randomly sampled population.

take_sample <- function(p, N){
  s <- sample(c(1, 0), N, replace = TRUE, prob = c(p, 1 - p))
  mean(s)
}

# Use the `set.seed` function to make sure your answer matches the expected result
# after random sampling
set.seed(1, sample.kind = "Rounding")

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Call the `take_sample` function to determine the sample average of `N`
# randomly selected people from a population containing a proportion of
# Democrats equal to `p`. Print this value to the console.

take_sample(p, N) # 0.46

# Exercise 2. Distribution of errors - part 1

# Assume the proportion of Democrats in the population p equals 0.45 and that the
# sample size N is 100 polled voters. The take_sample function you defined previously
# generates our estimate, X-bar.

# Replicate the random sampling 10,000 times and calculate p - X-bar for each random
# sample. Save these differences as a vector called errors. Find the average errors
# and plot a histogram of the distribution.

# The function take_sample that you defined in the previous exercise has already been
# run for you.

# Use the replicate function to replicate substracting the result of take sample from
# the value of p 10,000 times.

# Use the mean function to calculate the average of the differences between the sample
# average and actual value of p.

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people being polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result
# after random sampling
set.seed(1, sample.kind = "Rounding")

# Create an object called `errors` that replicates substracting the result of the
# `take_sample` function from `p` for `B` replications

errors <- replicate(B, {
  p - take_sample(p, N)
})

# Calculate the mean of the errors. Print this value to the console.

mean(errors)

# Exercise 3. Distribution of errors - part 2

# In the last exercise, you made a vector of differences between the actual value of p
# and an estimate, X-bar. We called these differences between the actual and estimated
# values `errors`.

# The `errors` object has already been loaded for you. Use the hist function to plot a
# histogram of the values contained in the vector `errors`. Which statement best describes
# the distribution of errors?

hist(errors)

# The errors are all about 0.05

# The errors are all about -0.05

# The errors are symmetrically distributed around 0 # Correct

# The errors range from -1 to 1


# Exercise 4. Average size of error

# The error p - X-bar is a random variable. In practice, the error is not observed
# because we do not know the actual proportion of Democratic voters, p. However, we
# can describe the size of the error by constructing a simulation.

# What is the average size of the error if we define the size by taking the absolute
# value |p - X-bar|?

# Use the sample code to generate `errors`, a vector of |p - X-bar|

# Calculate the absolute value of `errors` using the abs function

# Calculate the average of these values using the mean function

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result
# after random sampling
set.seed(1, sample.kind = "Rounding")

# We generated `errors` by substracting the estimate from the actual proportion of 
# Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the mean of the absolute value of each simulated error. Print this
# value to the console.

mean(abs(errors)) # 0.039

# Exercise 5. Standard deviation of the spread

# The standard error is related to the typical size of the error we make when predicting.
# We say size because, as we just saw, the errors are centered around 0. In that sense, 
# the typical error is 0. For mathematical reasons related to the central limit theorem,
# we actually use the standard deviation of errors rather than the average of the
# absolute values.

# As we have discussed, the standard error is the square root of the average squared
# distance (X-bar - p)^2. The standard deviation is defined as the square root of the
# distance squared.

# Calculate the standard deviation of the spread.

# Use the sample code to generate `errors`, a vector of |p - X-bar| 

# Use ^2 to square the distances

# Calculate the average squared distance using the mean function

# Calculate the square root of these values using the sqrt function

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people being polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be
# replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected
# result after random sampling
set.seed(1, sample.kind = "Rounding")

# We generated `errors` by substracting the estimate from the actual proportion
# of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the standard deviation of `errors`

sqrt(mean(abs(errors)^2)) # 0.049

# Exercise 6. Estimating the standard error

# The theory we just learned tells us what this standard deviation is going to be
# because it is the standard error of X-bar.

# Estimate the standard error given an expected value of 0.45 and a sample size of 100.

# Calculate the standard error using sqrt function

# Define `p` as the expected value equal to 0.45
p <- 0.45

# Define `N` as the sample size
N <- 100

# Calculate the standard error

sqrt(p * (1 - p) / N) # 0.049

# Exercise 7. Standard error of the estimate

# In practice, we don't know p, so we construct an estimate of the theoretical prediction
# based by pluggin in X-bar for p. Calculate the standard error of the estimate: SE^(X-bar)

# Simulate a poll X using the sample function.

# When using the sample function, create a vector using c() that contains all possible
# polling options where `1` indicates a Democratic voter and `0` indicates a Republican
# voter.

# When using the sample function, use replace = TRUE within the sample function to indicate
# that sampling from the vector should occur with replacement.

# When using the sample funcion, use prob = within the sample function to indicate the
# probabilities of selecting either element (0 or 1) within the vector of possibilities.

# Use the mean function to calculate the average of the simulated poll, X_bar.

# Calculate the standard error of the X_bar using the sqrt function and print the result.

# Define `p` as a proportion of Democratic voters to simulate
p <- 0.45

# Define `N` as the sample size
N <- 100

# Use the `set.seed` function to make sure your answer matches the expected result after
# random sampling
set.seed(1, sample.kind = "Rounding")

# Define `X` as a random sample of `N` voters with a probability of picking a Democrat (`1`)
# equal to `p`

X <- sample(c(1,0), N, replace = TRUE, prob = c(p, 1 - p))

# Define `X_bar` as the average sampled proportion

X_bar <- mean(X)

# Calculate the standard error of the estimate. Print the result to the console.

sqrt(X_bar * (1 - X_bar) / N) # 0.049

# Exercise 8. Plotting the standard error

# The standard error estimates obtained from the Monte Carlo simulation, the theoretical
# prediction, and the estimate of the theoretical preidction are all very close, which
# tells us that the theory is working. This gives us a practical approach to knowing the
# typical error we will make if we predict p with X-bar. The theoretical result gives
# us an idea of how large a sample size is required to obtain the precision we need. Earlier
# we learned that the largest standard errors occur for p = 0.5.

# Creat a plot of the largest standard error for N ranging from 100 to 5,000. Based on this
# plot, how large does the sample size have to be to have a standard error of about 1%?

N <- seq(100, 5000, length = 100)
p <- 0.5
se <- sqrt(p * (1 - p) / N)

plot(N, se)

# 100
# 500
# 2,500 # Correct
# 4,000

# Exercise 9. Distribution of X-hat

# For N = 100, the central limit theorem tells us that the distribution of X-hat is...

# practically equal to p

# approximately normal with expected value p and standard error sqrt(p(1-p)/N) # Correct

# approximately normal with expected value X-bar and standard error sqrt(X-bar(1-X-bar)/N)

# not a random variable

# Exercise 10. Distribution of errors

# We calculated a vector `errors` that contained, for each simulated sample, the difference
# between the actual value p and our estimate X-hat.

# The errors X-bar - p are:

# practically equal to 0

# approximately normal with expected value 0 and standard error sqrt(p(1-p)/N) # Correct

# approximately normal with expected value p and standard error sqrt(p(1-p)/N)

# not a random variable

# Exercise 11. Plotting the errors

# Make a qq-plot of the errors you generated previously to see if they follow a normal
# distribution

# Run the supplied code

# Use the qqnorm() function to produce a qq-plot of the errors

# Use the qqline function to plot a line showing a normal distribution

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result
# after random sampling
set.seed(1, sample.kind = "Rounding")

# Generate errors by substracting the estimate from the actual proportion of Democratic
# voters
errors <- replicate(B, p - take_sample(p, N))

# Generate a qq-plot of `errors` with a qq-line showing a normal distribution

# Exercise 12. Estimating the probability of a specific value of X-bar

# If p = 0.45 and N = 100, use the central limit theorem to estimate the probability
# that X-bar > 0.5

# Use pnorm to define the probability that a value will be greater then 0.5

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Calculate the probability that the estimated proportion of Democrats in the population
# is greater than 0.5. Print this value to the console.

1 - pnorm(0.5, p, sqrt(p * (1 - p) / N)) # 0.157

# Exercise 13. Estimating the probability of a specific error size

# Assume you are in a practical situation and you don't know p. Take a sample of size
# N = 100 and obtain a sample average of X-bar = 0.51.

# What is the CLT approximation for the probability that your error size is equal or
# larger than 0.01?

# Calculate the standard error of the sample average using the sqrt function.

# Use pnorm twice to define the probabilities that a value will be less than -0.01 or
# greater than 0.01.

# Combine these results to calculate the probability that the error size will be 0.01
# or larger.

# Define `N` as the number of people polled 
N <- 100

# Define `X_hat` as the sample average
X_hat <- 0.51

# Define `se_hat` as the standard error of the sample average

se_hat <- sqrt(X_hat * (1 - X_hat) / N)

# Calculate the probability that the error is 0.01 or larger

pnorm(X_hat - 0.01, X_hat, se_hat) + 1 - pnorm(X_hat + 0.01, X_hat, se_hat) # .8414

## SECTION 3. CONFIDENCE INTERVALS AND P-VALUES

# You will be able to:

# Calculate confidence internals of different sizes around an estimate.

# Understand that a confidence interval is a random interval with a given probability
# of falling on top of the parameter.

# Explain the concept of "power" as it relates to inference.

# Understand the relationship between p-values and confidence intervals and explain
# why reporting confidence intervals is often preferable.

# CONFINDENCE INTERVALS

# We can use statistical theory to compute the probability that a given interval contains
# the true parameter p.

# 95% confidence intervals are intervals constructed to have a 95% chance of including p.
# The margin of error is approximately a 95% confidence interval.

# The start and end of these confidence intervals are random variables.

# To calculate any size confidence interval, we need to calculate the value z for which
# Pr(-z <= Z <= z) equals the desired confidence. For example, a 99% confidence interval
# requires calculating z for Pr(-z <= Z <= z) = 0.99.

# For a confidence interval of size q, we solve for z = 1 - (1 - q) / 2

# To determine a 95% confidence interval, use z <- qnorm(0.975). This value is slightly
# smaller than 2 times the standard error.

# geom_smooth confidence interval example

# The shaded area around the curve is related to the concept of confidence intervals
library(tidyverse)
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
  ggplot(aes(year, temperature)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Average Yearly Temperatures in New Haven")

# Monte Carlo simulation of confidence intervals

# Note that to compute the exact 95% confidence interval, we would use qnorm(.975) *
# SE_hat instead of 2*SE_hat

p <- 0.45
N <- 1000
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p)) # generate N observations
X_hat <- mean(X) # calculate X_hat
SE_hat <- sqrt(X_hat * (1 - X_hat) / N) # caculate SE_hat, SE of the mean of N observations
c(X_hat - 2 * SE_hat, X_hat + 2 * SE_hat) # build interval of 2 * SE above and below mean

# Solving for z with qnorm
z <- qnorm(0.995) # calculate z to solve for 99% confidence interval
pnorm(qnorm(0.995)) # demonstrating that qnorm gives the z value for a given probability
pnorm(qnorm(1 - 0.995)) # demonstrating symmetry of 1 - qnorm
pnorm(z) - pnorm(-z) # demonstrating that this z value gives correct probability for interval

# A MONTE CARLO SIMULATION FOR CONFIDENCE INTERVALS

# We can run a Monte Carlo simulation to confirm that a 95% confidence interval contains the
# true value of p 95% of the time.

# A plot of confidence intervals from this simulation demonstrates that most intervals include
# p, but roughly 5% of intervals miss the true value o p.

# Monte Carlo simulation

# Note that to compute the exact 95% confidence interval, we would use qnorm(.975) * SE_hat
# instead of 2 * SE_hat

B <- 10000
inside <- replicate(B, {
  X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p))
  X_hat <- mean(X)
  SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
  between(p, X_hat - 2 * SE_hat, X_hat + 2 * SE_hat) # TRUE if p in confidence interval
})
mean(inside)

# THE CORRECT LANGUAGE

# The 95% confidence intervals are random, but p is not random

# 95% refers to the probability that the random interval falls on top of p

# It is technically incorrect to state that p has a 95% chance of being in between two
# values because that implies p is random

# POWER

# If we are trying to predict the result of an election, then a confidence interval that
# includes a spread of 0 (a tie) is not helpful.

# A confidence interval that includes a spread of 0 does not imply a close election, it
# means the sample size is too small.

# Power is the probability of detecting an effect when there is a true effect to find.
# Power increases as sample size increases, because larger sample size means smaller
# standard error.

# Confidence interval for the spread with sample size of 25

# Note that to compute the exact 95% confidence interval, we would use c(-qnorm(.975),
# qnorm(.975)) instead of 1.96
N <- 25
X_hat <- 0.48
(2 * X_hat - 1) + c(-2, 2) * 2* sqrt(X_hat * (1 - X_hat) / N)

# P-VALUES

# The null hypothesis is the hypothesis that there is no effect. In this case, the null
# hypothesis is that the spread is 0, or p = 0.5.

# The p-value is the probability of detecting an effect of a certain size or larger when
# the null hypothesis is true.

# We can convert the probability of seeing an observed value under the null hypothesis
# into a standard normal random variable. We compute the value of z that corresponds to
# the observed result, and then use that z to compute the p-value.

# If a 95% confidence interval does not include our observed value, then the p-value
# must be smaller than 0.05.

# It is preferable to report confidence intervals instead of p-values, as confidence
# intervals give information about the size of the estimate and p-values do not.

# Computing a p-value for observed spread of 0.02
N <- 100 # sample size
z <- sqrt(N) * 0.02 / 0.5 # spread of 0.02
1 - (pnorm(z) - pnorm(-z))

# ANOTHER EXPLANATION OF P-VALUES

# The p-value is the probability of observing a value as extreme or more extreme than
# the result given that the null hypothesis is true.

# In the context of the normal distribution, this refers to the probability of observing
# a Z-score whose absolute value is as high or higher than the Z-score of interest.

# Suppose we want to find the p-value of an obervation 2 standard deviations larger than
# the mean. This means we are looking for anything with |z| >= 2.

# Graphically, the p-value gives the probability of an observation that's at least as far
# away from the mean or further. This plot shows a standard normal distribution (centered
# at z = 0 with a standard deviation of 1). The shaded tails are the region of the graph
# that are 2 standard deviations or more away from the mean.

# The right tail can be found with 1 - pnorm(2). We want to have both tails, though,
# because we want to find the probability of any observation as far away from the mean
# or farther, in any direction. (This is what's meant by a two-tailed p-value.) Because
# the distribution is symmetrical, the right and left tails are the same size and we
# know that our desired value is just 2 * (1 - pnorm(2)).

# Recall that, by default, pnorm() gives the CDF for a normal distribution with a mean
# mu = 0 and standard deviation sigma = 1. To find p-values for a given z-score z in a
# normal distribution with mean mu and standard deviation sigma, use 2 * (1 - pnorm(z,
# mu, sigma)) instead.

# ASSESSMENT 3.1. CONFIDENCE INTERVALS AND P-VALUES

# Exercise 1. Confidence interval for p

# For the following exercises, we will use actual poll data from the 2016 election. The
# exercises will contain pre-loaded data from the dslabs package.
library(dslabs)
data("polls_us_election_2016")

# We will use all the national polls that ended within a few weeks before the election.

# Assume that there are only two candidates and construct a 95% confidence interval for
# the election night proportion p.

# Use filter to subset the data set for the poll data you want. Include polls that ended
# on or after October 31, 2016 (enddate). Only include polls that took place in the
# United States. Call this filtered object `polls`.

# Use nrow() to make sure you created a filtered object `polls` that contains the correct
# number of rows.

# Extract the sample size N from the first poll in your subset object `polls`.

# Convert the percentage of Clinton voters (`rawpoll_clinton`) from the first poll in
# `polls` to a proportion, X_hat. Print this value to the console.

# Find the standard error of X_hat given N. Print this result to the console.

# Calculate the 95% confidence interval of this estimate using the qnorm() function.

# Save the lower and upper confidence intervals as an object called `ci`. Save the lower
# confidence interval first.

# Generate an object `polls` that contains data filtered for polls that ended on or after
# October 31, 2016 in the United States

polls <- filter(polls_us_election_2016, enddate >= "2016-10-31" & state == "U.S.")

# How many rows does `polls` contain? Print this value to the console.

nrow(polls)

# Assign the first sample size of the first poll in `polls` to a variable called `N`.
# Print this value to the console.

N <- polls$samplesize[1]
N

# For the first poll in `polls`, assign the estimated percentage of Clinton voters
# to a variable called `X_hat`. Print this value to the console.

X_hat <- polls$rawpoll_clinton[1] / 100
X_hat

# Calculate the standard error of `X_hat` and save it to a variable called `se_hat`.
# Print this value to the console

se_hat <- sqrt(X_hat * (1 - X_hat) / N)
se_hat

# Use qnorm() to calculate the 95% confidence interval for the proportion of Clinton
# voters. Save the lower end and then the upper confidence interval to a variable
# called `ci`.

ci <- c(X_hat - qnorm(0.975) * se_hat, X_hat + qnorm(0.975) * se_hat)

# Exercise 2. Pollster results for p

# Create a new object called `pollster_results` that contains the pollster's name, the
# end data of the poll, the proportion of voters who declared a vote for Clinton, the
# standard error of this estimate, and the lower and upper bounds of the confidence
# interval for the estimate.

# Use the mutate function to define four new columns: X_hat, se_hat, lower, and upper.
# Temporarily add these columns to the polls object that has already been loaded for you.

# In the X_hat column, convert the raw poll results for Clinton to a proportion.

# In the se_hat column, calculate the standard error of X_hat for each poll using the
# sqrt function.

# In the lower column, calculate the lower bound of the 95% confidence interval using
# the qnorm() function

# In the upper column, calculate the upper bound of the 95% confidence interval using
# the qnorm() function

# Use the select function to select the columns from polls to save to the new object
# pollster_results.

# The `polls` object that filtered all the data by date and nation has already been
# loaded. Examine it using the `head` function.
head(polls)

# Create a new object called `pollster_results` that contains columns for pollster name,
# end date, X_hat, se_hat, lower confidence interval, and upper confidence interval for
# each poll.

pollster_results <- polls %>%
  mutate(X_hat = rawpoll_clinton / 100, se_hat = sqrt(X_hat * (1 - X_hat) / samplesize),
         lower = X_hat - qnorm(0.975) * se_hat, upper = X_hat + qnorm(0.975) * se_hat) %>%
  select(pollster, enddate, X_hat, se_hat, lower, upper)

# Exercise 3. Comparing to actual results - p

# The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column called
# `hit` to `pollster_results` that states if the confidence interval included the true
# porportion p = 0.482 or not. What proportion of confidence intervals included p?

# Finish the code to create a new object called `avg_hit` by following these steps.

# Use the mutate function to define a new variable called `hit`

# Use logical expressions to determine if each values in lower and upper span the actual
# proportion

# Use the mean function to determine the average value in hit and summarize the results
# using summarize.

# The `pollster_results` object has already been loaded. Examine it using the `head` function.
head(pollster_results)

# Add a logical variable called `hit` that indicates whether the actual value exists within
# the confidence interval of each poll. Summarize the average `hit` result to determine the
# proportion of polls with confidence intervals that include the actual value. Save the result
# as an object called `avg_hit`.

avg_hit <- pollster_results %>%
  mutate(hit = lower <= 0.482 & upper >= 0.482) %>%
  summarize(avg_hit = mean(hit)) # 0.31

# Exercise 4. Theory of confidence intervals

# If these confidence intervals are constructed correctly, and the theory holds up,
# what proportion of confidence intervals should include p?

# 0.05

# 0.31

# 0.50

# 0.95 # Correct

# Exercice 5. Confidence interval for d

# A much smaller proportion of the polls than expected produce confidence intervals
# containing p. Notice that most polls that fail to include p are underestimating.
# The rationale for this is that undecided voters historically divide evenly between
# two main candidates on election day.

# In this case, it is more informative to estimate the spread or the difference between
# the proportion of two candidates d, or 0.482 - 0.461 = 0.021 for this election.

# Assume that there are only two parties and that d = 2p - 1. Construct a 95% confidence
# interval for difference in proportinos on election night.

# Use the mutate function to define a new variable called `d_hat` in polls as the
# proportion of Clinton voters minus the proportion of Trump voters.

# Extract the sample size N from the first poll in your subset object polls.

# Extract the difference in proportions of voters d_hat from the first poll in your
# subset object polls.

# Use the formula above to calculate p from d_hat. Assign p to the variable X_hat.

# Find the standard error of the spread given N. Save this as se_hat.

# Calculate the 95% confidence interval of this estimate of the difference in proportions, 
# d_hat, using the qnorm() function.

# Save the lower and upper confidence intervals as an object called `ci`. Save the lower
# confidence interval first.

# Add a statement to this line of code that will add a new column named `d_hat` to `polls`.
# The new column should contain the difference in the proportion of voters.

polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.") %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)

# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this
# value to the console.

N <- polls$samplesize[1]
N

# Assign the difference `d_hat` of the first poll in `polls` to a variable called
# `d_hat`. Print this value to the console.

d_hat <- polls$d_hat[1]
d_hat

# Assign proportion of votes for Clinton to the variable `X_hat`.

X_hat <- (d_hat + 1) / 2

# Calculate the standard error of the spread and save it to a variable called
# `se_hat`. Print the value to the console.

se_hat <- 2 * sqrt(X_hat * (1 - X_hat) / N)
se_hat

# Use qnorm to calculate the 95% confidence interval for the difference in the proportions
# of voters. Save the lower end and then the upper confidence interval to a variable
# called `ci`.

ci <- c(d_hat - qnorm(0.975) * se_hat, d_hat + qnorm(0.975) * se_hat)

# Exercise 6. Pollster results for d

# Create a new object called `pollster_results` that contains the pollster's name, the
# end date of the poll, the difference in the proportion of voters who declared a vote
# either, and the lower and upper bounds of the confidence interval for the estimate.

# Use the mutate function to define four new columns: `X_hat`, `se_hat`, `lower`, and
# `upper`. Temporarily add these columns to the polls object that has already been loaded
# for you. 

# In the X_hat column, calculate the proportion of voters for Clinton using d_hat.

# In the s_hat column, calculate the standard error of the spread for each poll using
# the sqrt() function.

# In the lower column, calculate the lower bound of the 95% confidence interval using the
# qnorm function.

# In the upper column, calculate the upper bound of the 95% confidence interval using the
# qnorm function.

# Use the select function to select the pollster, enddate, d_hat, lower, upper columns from
# polls to save to the new object pollster_results.

# The subset `polls` data with `d_hat` already calculated has been loaded. Examine it using
# the `head` function.
head(polls)

# Create a new object called `pollster_results` that contains columns for pollster name,
# end date, d_hat, lower confidence interval of d_hat, and upper confidence interval
# of d_hat for each poll.

pollster_results <- polls %>%
  mutate(X_hat = (d_hat + 1) / 2, se_hat = 2 * sqrt(X_hat * (1 - X_hat) / samplesize),
         lower = d_hat - qnorm(0.975) * se_hat, upper = d_hat + qnorm(0.975) * se_hat) %>%
  select(pollster, enddate, d_hat, lower, upper)

# Exercise 7. Comparing to actual results - d

# What proportion of confidence intervals for the difference between the proportion of
# voters included d, the actual difference in election day?

# Use the mutate function to define a new variable within pollster_results called hit.

# Use logical expressions to determine if each values in lower and upper span the actual
# difference in proportions of voters.

# Use the mean function to determine the average value in hit and summarize the results
# using summarize.

# Save the result of your entire line of code as an object called avg_hit.

# The `pollster_results` object has already been loaded. Examine it using the `head`
# function
head(pollster_results)

# Add a logical variable called `hit` that indicates whether the actual value (0.021)
# exists within the confidence interval of each poll. Summarize the average `hit` result
# to determine the proportion of polls with confidence intervals that include the actual
# value. Save the result as an object called `avg_hit`.

avg_hit <- pollster_results %>%
  mutate(hit = lower <= 0.021 & upper >= 0.021) %>%
  summarize(avg_hit = mean(hit))

# Exercise 8. Comparing to actual results by pollster

# Although the proportion of confidence intervals that include the actual difference
# between the proportion of voters increases substantially, it is still lower than 0.95.
# In the next chapter, we learn the reason for this.

# To motivate our next exercises, calculate the difference between each poll's estimate
# d and the actual d = 0.021. Stratify this difference, or error, by pollster in a plot.

# Define a new variable `errors` that contains the difference between the estimated
# difference between the proportion of voters and the actual difference on election day,
# 0.021.

# To create a plot of errors by pollster, add a layer with the function geom_point. The
# aesthetic mappings require a definition of the x-axis and y-axis variables. So the code
# looks like the example below, but you fill in the variables for x and y.

# The last line of the example code adjusts the x-axis lables so that they are easier
# to read.

# Example code

# data %>% ggplot(aes(x = ??, y = ??)) +
#   geom_point() +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1))

# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)

# Add a variable called `error` to the object `polls` that contains the difference between
# d_hat and the actual difference on election day. Then make a plot of the error stratified
# by pollster.
polls %>% mutate(error = d_hat - 0.021) %>%
  ggplot(aes(x = pollster, y = error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Exercise 9. Comparing to actual results by pollster - multiple polls

# Remake the plot you made for the previous exercise, but only for pollsters that
# took five or more polls.

# You can use dplyr tools group_by and n to group data by variable of interest and then
# count the number of observations in the groups. The function filter filters data piped
# into it by your specified condition. For example:

# data %>% group_by(variable_for_grouping) %>%
#   filter(n() >= 5)

# Define a new variable errors that contains the difference between the estimated difference
# between the proportion of voters and the actual difference on election day, 0.021.

# Group the data by pollster using the group_by function.

# Filter the data by pollsters with 5 or more polls.

# Use ggplot to create the plot of errors by pollster.

# Add a layer with the function geom_point.

# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)

# Add a variable called `error` to the object `polls` that contains the difference
# between d_hat and the actual difference on election day. Then make a plot of the error
# stratified by pollster, but only for pollsters who took 5 or more polls.

polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>% filter(n() >= 5) %>%
  ggplot(aes(x = pollster, y = error)) +
  geom_point()

## SECTION 4. STATISTICAL MODELS

# You will be able to:

# Understand how aggregating data from different sources, as poll aggregators do for
# poll data, can improve the precision of a prediction.

# Understand how to fit a multilevel model to the data forecast, for example, election
# results.

# Explain why a simple aggregation of data is insufficient to combine results because
# of factors such as pollster bias.

# Use a data-driven model to account for additional types of sampling variability such
# as pollster-to-pollster variability.

# POLL AGGREGATORS

# Poll aggregators combine the results of many polls to simulate polls with a large
# sample size and therefore generate more precise estimates than individual polls.

# Polls can be simulated with a Monte Carlo simulation and used to construct an estimate
# of the spread and confidence intervals.

# The actual data science exercise of forecasting elections involves more complex statistical
# modeling, but these underlying ideas still apply.

# Simulating polls

# Note that to compute the exact 95% confidence interval, we would use qnorm(0.975) *
# SE_hat instead of 2 * SE_hat

d <- 0.039 # spread of Obama's victory in 2012
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d + 1) / 2

# calculate confidence intervals
confidence_intervals <- sapply(Ns, function(N){
  X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
  X_hat <- mean(X)
  SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
  2 * c(X_hat, X_hat - 2 * SE_hat, X_hat + 2 * SE_hat) - 1
})

# generate a data frame storing results
polls <- data.frame(poll = 1:ncol(confidence_intervals),
                    t(confidence_intervals), sample_size = Ns)
names(polls) <- c("poll", "estimate", "low", "high", "sample_size")
polls

# Calculating the spread of combined polls

# Note that to compute the exact 95% confidence interval, we would use qnorm(.975) instead
# of 1.96
library(tidyverse)
d_hat <- polls %>%
  summarize(avg = sum(estimate * sample_size) / sum(sample_size)) %>%
  .$avg
p_hat <- (1 + d_hat) / 2
moe <- 2 * 1.96 * sqrt(p_hat * (1 - p_hat) / sum(polls$sample_size))
round(d_hat * 100, 1)
round(moe * 100, 1)

# POLLSTERS AND MULTILEVEL MODELS

# Different poll aggregators generate different models of election results from the
# same poll data. This is because they use different statistical models.

# We will use actual polling data about the popular vote from the 2016 US presidential
# election to learn the principles of statistical modeling.

# POLL DATA AND POLLSTER BIAS

# We analyze real 2016 US polling data organized by FiveThirtyEight. We start by using
# reliable national polls taken within the week before the election to generate an urn
# model.

# Consider p the proportion voting for Clinton and 1 - p the proportion voting for Trump.
# We are interested in the spread d = 2p - 1.

# Poll results are a random normal variable with expected value of the spread d and
# standard error 2 * sqrt(p * (1 - p) / N)

# Our initial estimate of the spread did not include the actual spread. Part of the reason
# is that different pollsters have different numbers of polls in our dataset, and each
# pollster has a bias.

# Pollster bias reflects the fact that repeated polls by a given pollster have an
# expected value different from the actual spread and different from other pollsters.
# Each pollster has a different bias.

# The urn model does not account for pollster bias. We will develop a more flexible
# data-driven model that can account for effects like bias.

# Generating simulated poll data
library(dslabs)
data(polls_us_election_2016)
names(polls_us_election_2016)

# keep only national polls from the week before the election with a grade considered
# reliable
polls <- polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-10-31" &
           (grade %in% c("A+", "A", "A-", "B+") | is.na(grade)))

# add spread estimate
polls <- polls %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100)

# compute estimated spread for combined polls
d_hat <- polls %>%
  summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>%
  .$d_hat

# compute margin of error
p_hat <- (d_hat + 1) / 2
moe <- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))

# histogram of the spread
polls %>%
  ggplot(aes(spread)) +
  geom_histogram(color = "black", binwidth = 0.01)

# Investigating poll data and pollster bias

# number of polls per pollster in week before election
polls %>% group_by(pollster) %>% summarize(n())

# plot results by pollster with at least 6 polls
polls %>% group_by(pollster) %>%
  filter(n() >= 6) %>%
  ggplot(aes(pollster, spread)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# standard errors within each pollster
polls %>% group_by(pollster) %>%
  filter(n() >= 6) %>%
  summarize(se = 2 * sqrt(p_hat * (1 - p_hat) / median(samplesize)))

# DATA-DRIVEN MODELS

# Instead of using an urn model where each poll is a random draw from the same distribution
# of voters, we instead define a model using an urn that contains poll results from all
# possible pollsters.

# We assume the expected value of this model is the actual spread d = 2p - 1.

# Our new standard error sigma now factors in pollster-to-pollster variability. It can
# no longer be calculated from p or d and is an unknown parameter.

# The central limit theorem still works to estimate the sample average of many polls
# X1,...,XN because the average of the sum of many random variables is a normally 
# distributed random variable with expected value d and standard error sigma / sqrt(N).

# We can estimate the unobserved sigma as the sample standard deviation, which is
# calculated wit the sd function.

# Note that to compute the exact 95% confidence interval, we would use qnorm(.975) instead
# of 1.96.

# collect last result before the election for each pollster
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
  filter(enddate == max(enddate)) %>% # keep latest poll
  ungroup()

# histogram of spread estimates
one_poll_per_pollster %>%
  ggplot(aes(spread)) + geom_histogram(binwidth = 0.01)

# construct 95% confidence interval
results <- one_poll_per_pollster %>%
  summarize(avg = mean(spread), se = sd(spread) / sqrt(length(spread))) %>%
  mutate(start = avg - 1.96 * se, end = avg + 1.96 * se)
round(results * 100, 1)

# ASSESSMENT 4.1. STATISTICAL MODELS

# Exercise 1. Heights revisited

# We have been using urn models to motivate the use of probability models. However,
# most data science applications are not related to data obtained from urns. More
# common are data that come from individuals. Probability plays a role because the 
# data come from a random sample. The random sample is taken from a population and
# the urn serves as an analogy for the population.

# Let's revisit the heights dataset. For now, consider x to be the heights of all
# males in the dataset. Mathematically speaking, x is our population. Using the
# urn analogy, we have an urn with the value of x in it.

# What are the population average and standard deviation of our population?

# Execute the lines of code that create a vector x that contains heights for all
# males in the population.

# Calculate the average of x.

# Calculate the standard deviation of x.

# Load the `dslabs` package and data contained in `heights`.
library(dslabs)
data(heights)

# Make a vector of heights from all males in the population
library(tidyverse)
x <- heights %>% filter(sex == "Male") %>%
  .$height

# Calculate the population average. Print this value to the console.

mean(x)

# Calculate the population standard deviation. Print this value to the console.

sd(x)

# Exercise 2. Sample the population of heights

# Call the population average computed above `mu` and the standard deviation `sigma`.
# Now take a sample of size 50, with replacement, and construct an estimate for `mu` and
# `sigma`.

# Use the sample function to sample N values from x.

# Calculate the mean of the sampled heights.

# Calculate the standard deviation of the sampled heights.

# The vector of all male heights in our population `x` has already been loaded for you.
# You can examine the first six elements using `head`.
head(x)

# Use the `set.seed` function to make sure your answer matches the expected result after
# random sampling.
set.seed(1, sample.kind = "Rounding")

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`

X <- sample(x, N, replace = TRUE)

# Calculate the sample average. Print this value to the console.

mean(X)

# Calculate the sample standard deviation. Print this value to the console.

sd(X)

# Exercise 3. Sample and population averages

# What does the central limit theorem tell us about the sample average and how
# it is related to `mu`, the population average?

# It is identical to `mu`.

# It is a random variable with expected value `mu` and standard error `sigma` / sqrt(N) # Correct

# It is a random variable with expected value `mu` and standard error `sigma`.

# It understimates `mu`.

# Exercise 4. Confidence interval calculation

# We will use X_hat as our estimate of the heights in the population from our sample
# size N. We know from previous exercises that the standard estimate of our error
# X_hat - `mu` is `sigma` / sqrt(N).

# Construct a 95% confidence interval for `mu`.

# Use the sd and sqrt functions to define the standard error.

# Calculate the 95% confidence intervals using the qnorm function. Save the lower
# then the upper confidence interval to a variable called `ci`.

# The vector of all male heights in our population `x` has already been loaded for you.
# You can examine the first six elements using `head`.
head(x)

# Use the set.seed function to make sure your answer matches the expected result after
# random sampling.
set.seed(1, sample.kind = "Rounding")

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)

# Define `se` as the standard error of the estimate. Print this value to the console.

se <- sd(X) / sqrt(N)
se

# Construct a 95% confidence interval for the population average based on our sample.
# Save the lower then the upper confidence interval to a variable called `ci`

ci <- c(mean(X) - qnorm(.975) * se, mean(X) + qnorm(.975) * se)

# Exercise 5. Monte Carlo simulation for heights

# Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals
# as you have just done. What proportion of these intervals include `mu`?

# Use the replicate function to replicate the sample code for B <- 10000 simulations.
# Save the results of the replicated code to a variable called res. The replicated code
# should complete the following steps: (1) Use the sample function to sample N values
# from x. Save the sampled heights as a vector called X. (2) Create an object called
# interval that contains the 95% confidence interval for each of the samples. Use the
# same formula you used in the previous exercise to calculate this interval. (3) Use
# the between function to determine if `mu` is contained within the confidence interval
# of that simulation.

# Define `mu` as the population average
mu <- mean(x)

# Use the `set.seed` function to make sure your answer matches the expected result
# after random sampling.
set.seed(1, sample.kind = "Rounding")

# Define `N` as the number of people measured
N <- 50

# Define `B` as the number of times to run the model
B <- 10000

# Define an object `res` that contains a logical vector for simulated intervals
# that contain `mu`

res <- replicate(B, {
  X <- sample(x, N, replace = TRUE)
  interval <- c(mean(X) - qnorm(.975) * sd(X) / sqrt(N),
                mean(X) + qnorm(.975) * sd(X) / sqrt(N))
  between(mu, interval[1], interval[2])
})

# Calculate the proportion of results in `res` that include `mu`. Print this
# value to the console.

mean(res)

# Exercise 6. Visualizing polling bias

# In this section, we used visualization to motivate the presence of pollster
# bias in election polls. Here we will examine that bias more rigorously. Let's
# consider two pollsters that conducted daily polls and look at national polls
# for the month before the election.

# Is there a poll bias? Make a plot of the spreads for each poll.

# Use ggplot to plot the spread for each of the two pollsters.

# Define x- and y-axes using aes() within the ggplot function

# Use geom_boxplot() to make a boxplot of the data.

# Use geom_point() to add data points to the plot.

# Load the libraries and data you need for the following exercises
library(dslabs)
library(dplyr)
library(ggplot2)
data("polls_us_election_2016")

# These lines of code filter for the polls we want and calculate the spreads
polls <- polls_us_election_2016 %>%
  filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research",
                         "The Times-Picayune/Lucid") &
           enddate >= "2016-10-15" &
           state == "U.S.") %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100)

# Make a boxplot with points of the spread for each pollster

polls %>% ggplot(aes(pollster, spread)) +
  geom_boxplot() +
  geom_point()

# Exercise 7. Defining pollster bias

# The data do seem to suggest there is a difference between the pollsters. However,
# these data are subject to variability. Perhaps the difference we observe are due
# to chance. Under the urn model, both pollsters should have the same expected
# value: the election day difference, d.

# We will model the observed data Yij in the following way:

# Yij = d + bi + `epsilon`ij

# with i = 1,2 indexing the two pollsters, bi the bias for pollster i, and `epsilon`ij
# poll to poll chance variability. We assume the `epsilon` are independent from each
# other, have expected value 0 and standard deviation `sigma`i regardless of j.

# Which of the following statements best reflects what we need to know to determine
# if our data fit the urn model?

# Is `epsilon`ij = 0?

# How close are Yij to d?

# is b1 != b2? # Correct

# Are b1 = 0 and b2 = 0?

# Exercise 8. Derive expected value

# We modelled the observed data Yij as:

# Yij = d + bi + `epsilon`ij

# On the right side of this model, only `epsilon`ij is a random variable. The other
# two values are constants.

# What is the expected value of Y1j?

# d + b1 # Correct

# b1 + `epsilon`ij

# d

# d + b1 + `epsilon`ij

# Exercise 9. Expected value and standard error of poll 1

# Suppose we define Y-bar-1 as the average of poll results from the first poll and
# `sigma`-1 as the standard deviation of the first poll.

# What is the expected value and standard error of Y-bar-1?

# The expected value is d + b1 and the standard error is `sigma`-1

# The expected value is d and the standard error is `sigma`-1 / sqrt(N1)

# The expected value is d + b1 and the standard error is `sigma`-1 / sqrt(N1) # Correct

# The expected value is d and the standard error is `sigma`-1 / sqrt(N1)

# Exercise 10. Expected value and standard errpr of poll 2

# Now we define Y-bar-2 as the average of poll results from the second poll.

# What is the expected value and standard error of Y-bar-2?

# The expected value is d + b2 and the standard error is `sigma`-2

# The expected value is d and the standard error is `sigma`-2 / sqrt(N2)

# The expected value is d + b2 and the standard error is `sigma`-2 / sqrt(N2) # Correct

# The expected value is d and the standard error is `sigma`-2 + sqrt(N2)

# Exercise 11. Difference in expected values between polls

# Using what we learned by answering the previous questions, what is the expected
# value of Y-bar-2 - Y-bar-1?

# (b2 - b1)^2

# b2 - b1 / sqrt(N)

# b2 + b1

# b2 - b1 # Correct

# Exercise 12. Standard error of the difference between polls

# Using what we learned by answering the previous questions above, what is the 
# standard error of Y-bar-2 - Y-bar-1?

# sqrt(`sigma`-2^2 / N2 + `sigma`-1^2 / N2) # Correct

# sqrt(`sigma`-2 / N2 + `sigma`-1 / N1)

# (`sigma`-2^2 / N2 + `sigma`-1^2 / N1)^2

# `sigma`-2^2 / N2 + `sigma`-1^2 / N1

# Exercise 13. Compute the estimates

# The answer to the previous question depends on `sigma`-1 and `sigma`-2, which
# we don't know. We learned that we can estimate these values using the sample
# standard deviation.

# Compute the estimates of `sigma`-1 and `sigma`-2.

# Group the data by pollster.

# Summarize the standard deviation of the spreads for each of the two pollsters.
# Name the standard deviation s.

# Store the pollster names and standard deviations of the spreads (`sigma`) in an
# object called `sigma`.

# The `polls` data have already been loaded for you. Use the `head` function to 
# examine them.
head(polls)

# Create an object called `sigma` that contains a column for `pollster` and a
# column for `s`, the standard deviation of the spread.

sigma <- polls %>% group_by (pollster) %>%
  summarize(s = sd(spread))

# Print the contents of sigma to the console

sigma

# Exercise 14. Probability distribution of the spread

# What does the central limit theorem tell us about the distribution of the differences
# between the pollster averages, Y-bar-2 - Y-bar-1?

# The central limit theorem cannot tell us anything because this difference is not
# the average of a sample.

# Because Yij are approximately normal, the averages are normal too.

# If we assume N2 and N1 are large enough, Y-bar-2 and Y-bar-1, and their difference,
# are approximately normal. # Correct

# These data do not contain vectors of 0 and 1, so the central limit theorem does not
# apply.

# Exercise 15. Calculate the 95% confidence interval of the spreads

# We have constructed a random variable that has expected value b2 - b1, the pollster
# bias difference. If our model holds, then this random variable has an approximately
# normal distribution. The standard error of this random variable depends on `sigma`-1
# and `sigma`-2, but we can use the sample standard deviations we computed earlier. We
# have everything we need to answer our initial question: is b2 - b1 different from 0?

# Construct a 95% confidence interval for the difference between b2 and b1. Does this
# interval contain zero?

# Use pipes %>% to pass the data polls on to functions that will group by pollster and
# summarize the average spread, standard deviation, and number of polls per pollster.

# Calculate the estimate by substracting the average spreads. Save this estimate to a
# variable called estimate.

# Calculate the standard error using the standard deviations of the spreads and the
# sample size. Save this value to a variable called se_hat.

# Calculate the 95% confidence intervals using the qnorm function. Save the lower and
# then upper confidence interval to a variable called ci.

# The `polls` data have already been loaded for you. Use the `head` function to examine
# them.
head(polls)

# Create an object called `res` that summarizes the average, standad deviation, and
# number of polls for the two pollsters.

res <- polls %>% group_by(pollster) %>%
  summarize(avg = mean(spread), sd = sd(spread), n = n())

# Store the difference between the larger average and the smaller in a variable
# called `estimate`. Print this value to the console.

estimate <- max(res$avg) - min(res$avg)
estimate

# Store the standard error of the estimates as a variable called `se_hat`. Print this
# value to the console.

se_hat <- sqrt(res$sd[1]^2 / res$n[1] + res$sd[2]^2 / res$n[2])
se_hat

# Calculate the 95% confidence interval of the spreads. Save the lower end and then
# upper confidence interval to a variable called `ci`.

ci <- c(estimate - qnorm(.975) * se_hat, estimate + qnorm(.975) * se_hat)

# Exercise 16. Calculate the p-value

# The confidence interval tells us there is a relatively strong pollster effect
# resulting in a difference of about 5%. Random variablily does not seem to
# explain it. 

# Compute a p-value to relay the fact that chance does not explain the observed
# pollster effect.

# Use the pnorm function to calculate the probability that a random value is larger
# than the observed ratio of the estimate standard error.

# Multiply the probability by 2, because this is the two-tailed test.

# We made an object `res` to summarize the average, standard deviation, and number
# of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>%
  summarize(avg = mean(spread), s = sd(spread), N = n())

# The variables `estimate` and `se_hat` contain the spread estimates and standard
# error, respectively:
estimate <- res$avg[2] - res$avg[1]
se_hat <- sqrt(res$s[2]^2 / res$N[2] + res$s[1]^2 / res$N[1])

# Calculate the p-value

2 * (1 - pnorm(estimate / se_hat))

# Exercise 17. Comparing within-poll and between-poll variability

# We compute statistic called the t-statistic by dividing our estimate of b2 - b1 by
# its estimated standard error:

# Y-bar-2 - Y-bar-1 / sqrt(`sigma`-2^2 / N2 + `sigma`-1^2 / N1)

# Later we will learn of another approximation for the distribution of this statistic
# for values of N2 and N1 that aren't large enough for the CLT.

# Note that our data has more than two pollsters. We can also test for pollster effect
# using all posters, not just two. The idea is to compare the variability across polls
# to variability within polls. We can construct statistics to test for effects and
# approximate their distribution. The area of statistics that does this is called 
# Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very
# useful set of tools to answer questions such as: is there a pollster effect?

# Compute the average and standard deviation for each pollster and examine the variability
# across the averages and how it copares to the variability within the pollsters,
# summarized by the standard deviation.

# Group the polls data by pollster.

# Summarize the average and standard deviation of the spreads for each pollster.

# Create an object called `var` that contains three columns: pollster, mean spread,
# and standard deviation.

# Be sure to name the column for mean avg and the column for standard deviation s.

# Execute the following lines of code to filter the polling data and calculate the
# spread.
polls <- polls_us_election_2016 %>%
  filter(enddate >= "2016-10-15" &
           state == "U.S.") %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100) %>%
  ungroup()

# Create an object called `var` that contains columns for the pollster, mean spread,
# and standard deviation. Print the contents of the object to the console.

var <- polls %>% group_by(pollster) %>%
  summarize(avg = mean(spread), s = sd(spread))
var

## SECTION 5. BAYESIAN STATISTICS

# You will learn:

# Apply Bayes' theorem to calculate the probability of A given B.

# Understand how to use hierarchical models to make better predictions by considering
# multiple levels of variability.

# Compute a posterior probability using an empirical Bayesian approach.

# Calculate a 95% credible interval from a posterior probability.

# BAYESIAN STATISTICS

# In the urn model, it does not make sense to talk about the probability of p being
# greater than a certain value because p is a fixed value.

# With Bayesian statistics, we assume that p is in fact random, which allows us to
# calculate probabilities related to p.

# Hierarchical models describe variability at different levels and incorporate all
# these levels into a model for estimating p.

# BAYES' THEOREM

# Bayes' Theorem states that the probability of event A happening given event B is
# equal to the probability of both A and B divided by the probability of event B.

# Pr(A|B) = Pr(B|A) * Pr(A) / Pr(B)

# Bayes' Theorem shows that a test for a very rare disease will have a high percentage
# of false positives even if the accuracy of the test is high.

# Equations: Cystic fibrosis test probabilities

# In these probabilities, + represents a positive test, - represents a negative test,
# D = 0 indicates no disease, and D = 1 indicates the disease is present.

# Probability of having the disease given a positive test: Pr(D = 1|+)

# 99% test accuracy when disease is present: Pr(+|D = 1) = 0.99

# 99% test accuracy when disease is absent: Pr(-|D = 0) = 0.99

# Rate of cystic fibrosis: Pr(D = 1) = 0.00025

# Bayes' Theorem can be applied like this:

# Pr(D = 1|+) = Pr(+|D=1)*Pr(D=1) / Pr(+)

# Pr(D = 1|+) = Pr(+|D = 1) * Pr(D = 1) / Pr(+|D=1)*Pr(D=1)+Pr(+|D=0)*Pr(D=0)

# Substituting known values, we obtain:

# Pr(D = 1|+) = 0.99 * 0.00025 / 0.99*0.00025+0.01*0.99975 = 0.02

# Monte Carlo simulation

prev <- 0.00025 # disease prevalence
N <- 100000 # number of tests
outcome <- sample(c("Disease", "Healthy"), N, replace = TRUE, prob = c(prev, 1 - prev))

N_D <- sum(outcome == "Disease") # number with disease
N_H <- sum(outcome == "Healthy") # number healthy

# for each person, randomly determine if test is + or -
accuracy <- 0.99
test <- vector("character", N)
test[outcome == "Disease"] <- sample(c("+", "-"), N_D, replace = TRUE, prob = c(accuracy, 1 - accuracy))
test[outcome == "Healthy"] <- sample(c("-", "+"), N_H, replace = TRUE, prob = c(accuracy, 1 - accuracy))

table(outcome, test)

# BAYES IN PRACTICE

# The techniques we have used up until now are referred to as frequentist statistics as
# they consider only the frequency of outcomes in a dataset and do not include any outside
# information. Frequentist statistics allow us to compute confidence intervals and p-values.

# Frequentist statistics can have problems when sample sizes are small and when the data
# are extreme compared to historical results.

# Bayesian statistics allows prior knowledge to modify observed results, which alters our
# conclusions about event probabilities.

# THE HIERARCHICAL MODEL

# Hierarchical models use multiple levels of variability to model results. They are
# hierarchical because values in the lower levels of the model are computed using
# values from higher levels of the model.

# We model baseball player batting average using a hierarchical model with two levels
# of variability:

# p ~ N(`mu`, `tau`) describes player-to-player variability in natural ability to hit, 
# which has a mean `mu` and standard deviation `tau`.

# Y | p ~ N(p, `sigma`) describes a player's observed batting average given their ability
# p, which has a mean p and standard deviation `sigma` = sqrt(p*(1-p)/N). This represents
# variability due to luck.

# In Bayesian hierarchical models, the first level is called the prior distribution and
# the second level is called the sampling distribution.

# The posterior distribution allows us to compute the probability distribution of p given
# that we have observed data Y.

# By the continuous version of Bayes' rule, the expected value of the posterior distribution
# p given Y = y is a weighted average between the prior mean `mu` and the observed data Y:

# E(p|y) = B*`mu` + (1 - B)*Y where B = `sigma`^2 / `sigma`^2 + `tau`^2

# The standard error of the posterior distribution SE(p|Y)^2 is 
# 1 / (1/`sigma`^2) + (1/`tau`^2). Note that you will need to take the square root of both
# sides to solve for the standard error.

# This Bayesian approach is also known as shrinking. When `sigma` is large, B is close to
# 1 and our prediction of p shrinks toward the mean `mu`. When `sigma` is small, B is close
# to 0 and our prediction of p is more weighted toward the observed data Y.

# ASSESSMENT 5.1. BAYESIAN STATISTICS

# Exercise 1. Statistics in the courtroom

# In 1999 in England Sally Clark was found guilty of the murder of two of her sons. Both
# infants were found dead in the morning, one in 1996 and another in 1998, and she claimed
# the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm
# was found on the two infants so the main piece of evidence against her was the testimony
# of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS
# was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in
# 8,500 and then calculating that the chance of two SIDS cases was 8,500 x 8,500 ~= 73
# million.

# Based on what we've learned throughout this course, which statement best describes a
# potential flaw in Sir Meadow's reasoning?

# Sir Meadow assumed that the second death was independent of the first son being affected,
# thereby ignoring possible genetic causes. # Correct

# There is no flaw. The multiplicative rule always applies in this way:
# Pr(A & B) = Pr(A) * Pr(B)

# Sir Meadow should have added the probabilities: Pr(A & B) = Pr(A) + Pr(B)

# The rate of SIDS is too low to perform these types of statistics.

# Exercise 2. Recalculating the SIDS statistics

# Let's assume that there is in fact a genetic component and the probability of 
# Pr(second case of SIDS|first case of SIDS) = 1/100, is much higher than 1 in 8,500.

# What is the probability of both of Sally Clark's sons dying of SIDS?

# Calculate the probability of both sons dying of SIDS.

# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1 / 8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1 / 100

# Calculate the probability of both sons dying of SIDS. Print this value to the console.

Pr_1 * Pr_2

# Exercise 3. Bayes' rule in the courtroom

# Many press reports stated that the expert claimed the probability of Sally Clark being
# innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony
# this way. This probability can be written like this:

# Pr(mother is a murderer|two children found dead with no evidence of harm)

# Bayes' rule tells us this probability is equal to:

# Pr(two children found dead...)*Pr(mother is a murderer) / Pr(two children found dead...)

# Pr(two children found dead...)*Pr(mother is a murderer)

# Pr(two children found dead...|mother is a murderer)*Pr(mother is a murderer) /
# Pr(two children found dead...) # Correct

# 1 / 8500

# Exercise 4. Calculate the probability

# Assume that the probability of a murderer finding a way to kill her two children without
# leaving evidence of physical harm is:

# Pr(two children found dead...|mother is a murderer) = 0.50

# Assume that the murder rate among mothers is 1 in 1,000,000.

# Pr(mother is a murderer) = 1 / 1,000,000

# According to Bayes' rule, what is the probability of:

# Pr(mother is a murderer|two children are found dead...)?

# Use Bayes' rule to calculate the probability that the mother is a murderer, considering
# the rates of murdering mothers in the population, the probability that two siblings die
# of SIDS, and the probability that a murderer kills children without leaving evidence
# of physical harm.

# Print the result to the console.

# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1 / 8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1 / 100

# Define `Pr_B` as the probability of both sons dying of SIDS
Pr_B <- Pr_1 * Pr_2

# Define `Pr_A` as the rate of mothers that are murderers
Pr_A <- 1 / 1000000

# Define `Pr_BA` as the probability that two children die without evidence of harm, given
# that their mother is a murderer
Pr_BA <- 0.50

# Define `Pr_AB` as the probability that a mother is a murderer, given that her two children
# died with no evidence of physical harm. Print this value to the console.

Pr_BA * Pr_A / Pr_B

# Exercise 5. Misuse of statistics in the courts

# After Sally Clark was found guilty, the Royal Statistical Society issued a statement
# saying that there was "no statistical basis" for the expert's claim. They expressed
# concern at the "misuse of statistics in the courts". Eventually, Sally Clark was
# acquitted in June 2003.

# In addition to misuing the multiplicative rule as we saw earlier, what else did Sir
# Meadow miss?

# He made an arithmetic error in forgetting to divide by the rate of SIDS in siblings.

# He did not take into account how rare it is for a mother to murder her children. # Correct

# He mixed up the numerator and denominator of Bayes' rule.

# He did not take into account murder rates in the population.

# Exercise 6. Back to election polls

# Florida is one of the most closely watched states in the U.S. election because it has
# many electoral votes and the election is generally close. Create a table with the poll
# spread results from Florida taken during the last days before the election using the
# sample code.

# The CLT tells us that the average of these spreads is approximately normal. Calculate
# the spread average and provide an estimate of the standard error.

# Calculate the average of the spreads. Call this average avg in the final table.

# Calculate an estimate of the standard error of the spreads. Call this standard
# error se in the final table.

# Use the mean and sd functions nested within summarize to find the average and standard
# deviation of the grouped spread data.

# Save your results in an object called results.

# Load the libraries and poll data
library(dplyr)
library(dslabs)
data(polls_us_election_2016)

# Create an object `polls` that contains the spread of predictions for each candidate
# in Florida during the last polling days
polls <- polls_us_election_2016 %>%
  filter(state == "Florida" & enddate >= "2016-11-04") %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100)

# Examine the `polls` object using the `head` function
head(polls)

# Create an object called `results` that has two columns containing the average spread
# (`avg`) and the standard error (`se`). Print the results to the console.

results <- polls %>%
  summarize(avg = mean(spread), se = sd(spread) / sqrt(n()))
results

# Exercise 7. The prior distribution

# Assume a Bayestian model sets the prior distribution for Florida's election night
# spread d to be normal with expected value `mu` and standard deviation `tau`.

# What are the interpretations of `mu` and `tau`?

# `mu` and `tau` are arbitrary numbers that let us make probability statements about d.

# `mu` and `tau` summarize what we would predict for Florida before seeing any polls. # Correct

# `mu` and `tau` summarize what we want to be true. We therefore set `mu` at 0.10
# and `tau` at 0.01.

# The choice of prior has no effect on the Bayesian analysis.

# Exercise 8. Estimate the posterior distribution

# The CLT tells us that our estimate of the spread d-hat has a normal distribution with
# expected value d and standard deviation `sigma`, which we calculated in a previous
# exercise.

# Use the formulas for the posterior distribution to calculate the expected value of
# the posterior distribution if we set `mu` = 0 and `tau` = 0.01.

# Define `mu` and `tau`

# Identify which elements stored in the object results represent `sigma` and Y

# Estimate B using `sigma` and `tau`

# Estimate the posterior distribution using B, `mu`, and Y

# Estimate B using `sigma` and `tau`

# Estimate the posterior distribution using `B`, `mu`, and `tau`

# The `results` object have already been loaded. Examine the values stored: `avg` and
# `se` of the spread
results

# Define `mu` and `tau`
mu <- 0
tau <- 0.01

# Define a variable called `sigma` that contains the standard error in the object
# `results`

sigma <- results$se

# Define a variable called `Y` that contains the average in the object `results`

Y <- results$avg

# Define a variable `B` using the `sigma` and `tau`. Print this value to the console.

B <- sigma^2 / (sigma^2 + tau^2)

# Calculate the expected value of the posterior distribution

B * mu + (1 - B) * Y

# Exercise 9. Standard error of the posterior distribution

# Compute the standard error of the posterior distribution.

# Using the variable we have defined so far, calculate the standard error of the posterior
# distribution.

# Print this value to the console.

# Here are the variables we have define
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

# Compute the standard error of the posterior distribution. Print this value to the
# console.

sqrt(1 / (1 / sigma^2 + 1 / tau^2))

# Constructing a credible interval

# Using the fact that the posterior distribution is normal, create an interval that has
# a 95% of occurring centered at the posterior expected value. Note that we call these
# credible intervals.

# Calculate the 95% credibel intervals using the qnorm function

# Save the lower and upper confidence intervals as an object called ci. Save the lower
# confidence interval first.

# Here are the variables we have defined in previous exercises.
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
se <- sqrt(1 / (1 / sigma^2 + 1 / tau^2))

# Construct the 95% credible interval. Save the lower and then upper confidence
# interval to a variable called `ci`.
# B*`mu` + (1 - B)*Y

ci <- c((B*mu + (1 - B)*Y) - qnorm(.975) * se, (B*mu + (1 - B)*Y) + qnorm(.975) * se)

# Exercise 11. Odds of winning Florida

# According to this analysis, what was the possibility that Trump wins Florida?

# Using the pnorm function, calculate the probability that the spread in Florida was
# less than 0

# Assign the expected value of the posterior distribution to the variable `exp_value`
exp_value <- B * mu + (1 - B) * Y

# Assign the standard error of the posterior distribution to the variable `se`
se <- sqrt(1 / (1 / sigma^2 + 1 / tau^2))

# Using the `pnorm` function, calculate the probability that the actual spread was less
# than 0 (in Trump's favor). Print this value to the console.

pnorm(0, exp_value, se)

# Exercise 12. Change the priors

# We had set the prior variance `tau` to 0.01, reflecting that these races are often
# close.

# Change the prior variance to include values ranging from 0.005 to 0.05 and observe
# how the probability of Trump winning Florida changes making a plot.

# Create a vector of values of taus by executing the sample code.

# Create a function using function called p_calc that takes the value tau as the only
# argument, then calculates B from tau and sigma, and then calculates the probability
# of Trump winning, as we did in the previous exercise.

# Apply your p_calc function across all the new values of taus.

# Use the plot function to plot `tau` on the x-axis and the new probabilities on the
# y-axis

# Define the variables from previous exercises
mu <- 0
sigma <- results$se
Y <- results$avg

# Define a variable `taus` as different values of tau
taus <- seq(0.005, 0.05, len = 100)

# Create a function called `p_calc` that generates `B` and calculates the probability
# of the spread being less than 0.

p_calc <- function(tau){
  B <- sigma^2 / (sigma^2 + tau^2)
  exp_value <- B * mu + (1 - B) * Y
  se <- sqrt(1 / (1 / sigma^2 + 1 / tau^2))
  pnorm(0, exp_value, se)
}

# Create a vector called `ps` by applying the function `p_calc` across values in `taus`

ps <- sapply(taus, p_calc)

# Plot `taus` on the x-axis

plot(taus, ps)

## SECTION 6. ELECTION FORECASTING

# You will be able to:

# Understand how pollsters use hierarchical models to forecast the results of elections.

# Incorporate multiple sources of variability into a mathematical model to make
# predictions.

# Construct confidence intervals that better model deviations such as those seen in
# election data using the t-distribution.

# ELECTION FORECASTING

# In our model:

# The spread d ~ N(`mu`, `tau`) describes our best guess in the absence of polling data.
# We set `mu` = 0 and `tau` = 0.035 using historical data.

# The average of observed data X-bar | d ~ N(d, `sigma`) describes randomness due to
# sampling and the pollster effect.

# Because the posterior distribution is normal, we can report a 95% credible interval
# that has a 95% chance of overlapping the parameter using E(p | Y) and SE(p | Y).

# Given an estimate of E(p | Y) and SE(p | Y), we can use pnorm to compute the probability
# that d > 0.

# It is common to see a general bias that affects all pollsters in the same way. This
# bias cannot be predicted or measured before the election. We will include a term
# in later models to account for this variability.

# Definition of results object

# This code defines the `results` object used for empirical Bayes election forecasting.

library(tidyverse)
library(dslabs)
polls <- polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-10-31" &
           (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100)

one_poll_per_pollster <- polls %>% group_by(pollster) %>%
  filter(enddate == max(enddate)) %>%
  ungroup()

results <- one_poll_per_pollster %>%
  summarize(avg = mean(spread), se = sd(spread) / sqrt(length(spread))) %>%
  mutate(start = avg - 1.96 * se, end = avg + 1.96 * se)

# Computing the posterior mean, standard error, credible interval, and probability

# Note that to compute the exact 95% credible interval, we would use qnorm(.975)
# instead of 1.96

mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B * mu + (1 - B) * Y
posterior_se <- sqrt(1 / (1 / sigma^2 + 1 / tau^2))

posterior_mean
posterior_se

# 95% credible interval

posterior_mean + c(-1.96, 1.96) * posterior_se

# probability of d > 0

1 - pnorm(0, posterior_mean, posterior_se)

# MATHEMATICAL REPRESENTATION OF MODELS

# If we collect several polls with measured spreads X1,...,Xj with a sample size of N, 
# these random variables have expected value d and standard error 2 * sqrt(p * (1-p) / N).

# We represent each measurement as Xij = d + b + hi + `espsilon`ij where:

# The index i represents the different pollsters
# The index j represents the different polls
# Xij is the jth poll by the ith pollster
# d is the actual spread of the election
# b is the general bias affecting all posters
# hi represents the house effect for the ith pollster
# `epsilon`ij represents the random error associated with the i,jth poll

# The sample average is now X-bar = d + b + 1/N + sum(i = 1...N, Xi) with standard
# deviation SE(X-bar) = sqrt(`sigma`^2 / N + `sigma`b^2)

# The standard error of the general bias `sigma`b does not get reduced by averaging
# multiple polls, which increases the variability of our final estimate.

# Simulated data with Xj = d + `epsilon`j

J <- 6
N <- 2000
d <- 0.021
p <- (d + 1) / 2
X <- d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))

# Simulated data with Xi,j = d + `espsilon`i,j

I <- 5
J <- 6
N <- 2000
d <- 0.021
p <- (d + 1) / 2
X <- sapply(1:I, function(i){
  d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})

# Simulated data with Xi,j = d + hi + `epsilon`i,j

I <- 5
J <- 6
N <- 2000
d <- 0.021
p <- (d + 1) / 2
h <- rnorm(I, 0, 0.025) # assume standard error of pollster to pollster variability is 0.025
X <- sapply(1:I, function(i){
  d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})

# Calculating probability of d > 0 with general bias

# Note that `sigma` now includes an estimate of the variability due to general bias
# `sigma`b = 0.025

mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + 0.025^2)
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

posterior_mean <- B * mu + (1 - B) * Y
posterior_se <- sqrt(1 / (1 / sigma^2 + 1 / tau^2))

1 - pnorm(0, posterior_mean, posterior_se)

# PREDICTING THE ELECTORAL COLLEGE

# In the US election, each state has a certain number of votes that are won all-or-
# nothing based on the popular vote result in that state (with minor exceptions not
# discussed here)

# We use the left_join() function to combine the number of electoral votes with our
# poll results.

# For each state, we apply a Bayesian approach to generate an Election Day d. We keep
# our prior simple by assuming an expected value of 0 and a standard deviation based
# on recent history of 0.02.

# We can run a Monte Carlo simulation that for each iteration simulates poll results
# in each state using that state's average and standard deviation, awards electoral
# college votes for each state to Clinton if the spread is greater than 0, then
# compares the number of electoral votes won to the number of votes required to win
# the election (over 269).

# If we run a Monte Carlo simulation for the electoral college without accounting
# for general bias, we overestimate Clinton's chances of winning at over 99%.

# If we include a general bias term, the estimated probability of Clinton winning
# decreases significantly.

# Top 5 states ranked by electoral votes

# The results_us_election_2016 object is defined in the dslabs package.

library(tidyverse)
library(dslabs)
data(polls_us_election_2016)
head(results_us_election_2016)

results_us_election_2016 %>% arrange(desc(electoral_votes)) %>% top_n(5, electoral_votes)

# Computing the average and standard deviation of each state

results <- polls_us_election_2016 %>%
  filter(state != "U.S." &
           !grepl("CD", state) &
           enddate >= "2016-10-31" &
           (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100) %>%
  group_by(state) %>%
  summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
  mutate(state = as.character(state))

# 10 closest races = battleground states

results %>% arrange(abs(avg))

# joining electoral college votes and results

results <- left_join(results, results_us_election_2016, by = "state")

# states with no polls: note Rhode Island and District of Columbia = Democrat

results_us_election_2016 %>% filter(!state %in% results$state)

# assigns sd to states with just one poll as median of other sd values

results <- results %>%
  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))

# Calculating the posterior mean and posterior standard error

mu <- 0
tau <- 0.02
results %>% mutate(sigma = sd / sqrt(n),
                   B = sigma^2 / (sigma^2 + tau^2),
                   posterior_mean = B * mu + (1 - B) * avg,
                   posterior_se = sqrt(1 / (1 / sigma^2 + 1 / tau^2))) %>%
  arrange(abs(posterior_mean))

# Monte Carlo simulation of Election Night results (no general bias)

mu <- 0
tau <- 0.02
clinton_EV <- replicate(1000, {
  results %>% mutate(sigma = sd / sqrt(n),
                     B = sigma^2 / (sigma^2 + tau^2),
                     posterior_mean = B * mu + (1 - B) * avg,
                     posterior_se = sqrt(1 / (1 / sigma^2 + 1 / tau^2)),
                     simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                     clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>% # award votes
    summarize(clinton = sum(clinton)) %>% # total votes for clinton
    .$clinton + 7 # 7 votes for RI and DC
})
mean(clinton_EV > 269) # over 269 votes wins the election

# histogram of outcomes
data.frame(clinton_EV) %>%
  ggplot(aes(clinton_EV)) +
  geom_histogram(binwidth = 1) +
  geom_vline(xintercept = 269)

# Monte Carlo simulation including general bias

mu <- 0
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
  results %>% mutate(sigma = sqrt(sd^2/(n) + bias_sd^2), # added bias_sd term
                     B = sigma^2 / (sigma^2 + tau^2),
                     posterior_mean = B * mu + (1 - B) * avg,
                     posterior_se = sqrt(1 / (1 / sigma^2 + 1 / tau^2)),
                     simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                     clinton = ifelse(simulated_result >0, electoral_votes, 0)) %>%
    summarize(clinton = sum(clinton)) %>%
    .$clinton + 7
})
mean(clinton_EV_2 > 269)

# FORECASTING

# In poll results, p is not fixed over time. Variability within a single pollster comes
# from time variation.

# In order to forecast, our model must include a bias term bt to model the time effect.

# Pollsters also try to estimate f(t), the trend of p given time t using a model like:

# Yi,j,t = d + b + hi + bt + f(t) + `epsilon`i,j,t

# Once we decide on a model, we can use historical data and current data to estimate
# the necessary paramaters to make predictions.

# Variability across one pollster

# select all national polls by one pollster
one_pollster <- polls_us_election_2016 %>%
  filter(pollster == "Ipsos" & state == "U.S.") %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100)

# the observed standard error is higher than theory predicts
se <- one_pollster %>%
  summarize(empirical = sd(spread),
            theoretical = 2 * sqrt(mean(spread) * (1 - mean(spread)) / min(samplesize)))
se

# the distribution of the data is not normal
one_pollster %>% ggplot(aes(spread)) +
  geom_histogram(binwidth = 0.01, color = "black")

# Trend across time for several pollsters

polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-07-01") %>%
  group_by(pollster) %>%
  filter(n() >= 10) %>%
  ungroup() %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100) %>%
  ggplot(aes(enddate, spread)) +
  geom_smooth(method = "loess", span = 0.1) +
  geom_point(aes(color = pollster), show.legend = FALSE, alpha = 0.6)

# Plotting raw percentages across time

polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-07-01") %>%
  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
  gather(candidate, percentage, -enddate, -pollster) %>%
  mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
  group_by(pollster) %>%
  filter(n() >= 10) %>%
  ungroup() %>%
  ggplot(aes(enddate, percentage, color = candidate)) +
  geom_point(show.legend = FALSE, alpha = 0.4) +
  geom_smooth(method = "loess", span = 0.15) +
  scale_y_continuous(limits = c(30, 50))

# ASSESSMENT 6.1. ELECTION FORECASTING

# Exercise 1. Confidence intervals of polling data

# For each poll in the polling data set, use the CLT to create a 95% confidence
# interval for the spread. Create a new table called cis that contains columns
# for the lower and upper limits of the confidence intervals.

# Use pipes to pass the poll object on to the mutate function, which creates new
# variables.

# Create a variable called `X_hat` that contains the estimate of the proportion
# of Clinton voters for each poll.

# Create a variable called `se` that contains the standard error of the spread.

# Calculate the confidence intervals using the qnorm function and your calculated se.

# Use the select function to keep the following columns: state, stardate, enddate,
# pollster, grade, spread, lower, upper.

# Load the libraries and data
library(dplyr)
library(dslabs)
data(polls_us_election_2016)

# Create a table called `polls` that filters by state, date, and reports the spread.
polls <- polls_us_election_2016 %>%
  filter(state != "U.S." & enddate >= "2016-10-31") %>%
  mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100)

# Create an object called `cis` that has the columns indicated in the instructions

cis <- polls %>%
  mutate(X_hat = (spread + 1) / 2,
         se = 2 * sqrt(X_hat * (1 - X_hat) / samplesize),
         lower = spread - qnorm(.975) * se,
         upper = spread + qnorm(.975) * se) %>%
  select(state, startdate, enddate, pollster, grade, spread, lower, upper)

# Exercise 2. Compare to actual results

# You can add the final result to the cis table you just created using the left_join
# function as shown in the sample code.

# Now determine how often the 95% confidence interval includes the actual result.

# Create an object called `p_hits` that contains the proportion of intervals that
# contain the actual spread using the following two steps.

# Use the mutate function to create a new variable called `hit` that contains a logical
# vector for whether the actual_spread falls between the lower and upper confidence
# intervals.

# Summarize the proportion of values in `hit` that are true using the mean function
# inside of summarize.

# Add the actual results to the `cis` data set
add <- results_us_election_2016 %>%
  mutate(actual_spread = clinton / 100 - trump / 100) %>%
  select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>%
  left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of confidence
# intervals that contain the actual value. Print this object to the console.

p_hits <- ci_data %>%
  mutate(hit = actual_spread >= lower & actual_spread <= upper) %>%
  summarize(p_hits = mean(hit))
p_hits

# Exercise 3. Stratify by pollster and grade

# Now find the proportion of hits for each pollster. Show only pollsters with at least
# 5 polls and order them from best to worst. Show the number of polls conducted by each
# pollster and the FiveThirtyEight grade of each pollster.

# Create an object called `p_hits` that contains the proportion of intervals that contain
# the actual spread using the following steps.

# use the mutate function to create a new variable called hit that contains a logical
# vector for whether the actual_spread falls between the lower and upper confidence
# intervals.

# use the group_by function to group the data by pollster.

# Use the filter function to filter for pollsters that have at least 5 polls.

# Summarize the proportion of values in hit that are true as a variable called
# proportion_hits. Also, create new variables for the number of polls by each pollster (n)
# using the n() function and the grade of each poll (grade) by taking the first row of the
# grade column.

# Use the arrange function to arrange the proportion_hits in descending order.

# The `cis` data have already been loaded for you.
add <- results_us_election_2016 %>%
  mutate(actual_spread = clinton / 100 - trump / 100) %>%
  select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>%
  left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each
# pollster that has at least five polls.

p_hits <- ci_data %>%
  mutate(hit = actual_spread >= lower & actual_spread <= upper) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  summarize(proportion_hits = mean(hit),
            n = n(),
            grade = grade[1]) %>%
  arrange(desc(proportion_hits))

# Exercise 4. Stratify by state

# Repeat the previous exercise, but instead of pollster, stratify by state. Here
# we can't show grades.

# Create an object `p_hits` that contains the proportion of intervals that contain
# the actual spread using the following steps.

# Use the mutate function to create a new variable called `hit` that contains a logical
# vector for whether the actual_spread falls between the lower and upper confidence
# intervals.

# Use the group_by function to group the data by state.

# Use the filter function to filter for states that have more than 5 polls.

# Summarize the proportion of values in `hit` that are true as a variable called
# proportion_hits. Also create new variables for the number of polls in each state
# using the n() function.

# Use the arrange function to arrange the proportion_hits in descending order.

# The `cis` data have already been loaded for you.
add <- results_us_election_2016 %>%
  mutate(actual_spread = clinton / 100 - trump / 100) %>%
  select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>%
  left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each
# state that has more than 5 polls.

p_hits <- ci_data %>%
  mutate(hit = actual_spread >= lower & actual_spread <= upper) %>%
  group_by(state) %>%
  filter(n() > 5) %>%
  summarize(proportion_hits = mean(hit),
            n = n()) %>%
  arrange(desc(proportion_hits))

# Exercise 5. Plotting prediction results

# Make a barplot based on the result of the previous exercise.

# Reorder the states in order of the proportion of hits.

# Using ggplot, set the aesthetic with state as the x-variable and proportion of hits
# as the y-variable.

# Use geom_bar to indicate that we want to plot a barplot. Specify stat = "identity"
# to indicate that the height of the bar should match the value.

# Use coord_flip to flip the axes so the states are displayed from top to bottom and
# proportions are displayed from left to right.

# The `p_hits` data have already been loaded for you. Use the `head` function to
# examine it.
head(p_hits)

# Make a barplot of the proportion of hits for each state
library(ggplot2)

p_hits %>% arrange(proportion_hits) %>%
  ggplot(aes(state, proportion_hits)) +
  geom_bar(stat = "identity") +
  coord_flip()

# Exercise 6. Predicting the winner

# Even if a forecaster's confidence interval is incorrect, the overall predictions will
# do better if they correctly called the right winner.

# Add two columns to the cis table by computing, for each poll, the difference between
# the predicted spread and the actual spread, and define a column hit that is true if
# the signs are the same.

# Use the mutate function to add two new variables to the `cis` object: error and hit.

# For the error variable, substract the actual spread from the spread.

# For the hit variable, return "TRUE" if the poll predicted the actual winner. Use the
# sign function to check if their signs match.

# Save the new table as an object called `errors`.

# Use the tail function to examine the last 6 rows of errors.

# The `cis` data have already been loaded. Examine it using the `head` function
head(cis)

# Create an object called `errors` that calculates the difference between the predicted
# and actual spread and indicates if the correct winner was predicted.

cis <- ci_data # ci_data was converted to cis in DataCampa to include actual_spread

errors <- cis %>%
  mutate(error = spread - actual_spread,
         hit = sign(spread) == sign(actual_spread))

# Examine the last 6 rows of `errors`

tail(errors)

# Exercise 7. Plotting prediction results

# Create an object called `p_hits` that contains the proportion of instances when the
# sign of the actual spread matches the predicted spread for states with 5 or more polls.

# Make a barplot based on the result from the previous exercise that shows the proportion
# of times the sign of the spread matched the actual result for the data in p_hits.

# Use the group_by function to group the data by state.

# Use the filter function to filter for states that have 5 or more polls.

# Summarize the proportion of values in `hit` that are true as a variable called
# `proportion_hits`. Also create a variable called `n` for the number of polls in each
# state using the n() function.

# To make the plot, follow these steps:

# Reorder the states in order of the proportion of hits.

# Using ggplot, set the aesthetic with state as the x-variable and proportion of hits as the
# y-variable.

# Use geom_bar to indicate that we want to plot a barplot.

# Use coord_flip to flip the axes so the states are displayed from top to bottom and
# proportions are displayed from left to right.

# Create an object called `errors` that calculates the difference between the predicted
# and actual spread and indicates if the correct winner was predicted.
errors <- cis %>%
  mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))

# Create an object called `p_hits` that summarizes the proportion of hits for each state
# that has 5 or more polls.

p_hits <- errors %>%
  group_by(state) %>%
  filter(n() >= 5) %>%
  summarize(proportion_hits = mean(hit),
            n = n())

# Make a barplot of the proportion of hits for each state

p_hits %>% arrange(proportion_hits) %>%
  ggplot(aes(state, proportion_hits)) +
  geom_bar(stat = "identity") +
  coord_flip()

# Exercise 8. Plotting the errors

# In the previous graph, we see that most states' polls predicted the correct winner
# 100% of the time. Only a few states' polls were incorrect more than 25% of the time.
# Wisconsin got every single poll wrong and Michigan, more than 90% of the polls had the
# signs wrong.

# Make a histogram of the errors. What is the median of these errors?

# Use the hist function to generate a histogram of the errors

# Use the median function to compute the median error

# The `errors` data have already been loaded. Examine them using the `head` function
head(errors)

# Generate a histogram of the error

hist(errors$error)

# Calculate the median of the errors. Print this value to the console.

median(errors$error)

# Exercise 9. Plot bias by state

# We see that, at the state level, the median error was slightly in favor of Clinton.
# The distribution is not centered at 0, but at 0.037. This value represents the
# general bias we described in an earlier section.

# Create a boxplot to examine if the bias was general to all states or if it affected
# some states differently. Filter the data to include only pollsters with grades B+ or
# higher.

# Use the filter function to filter the data for polls with grades equal to A+, A, A-, 
# or B+.

# Use the reorder function to order the state data by error

# Using ggplot, set the aesthetic with state as the x-variable and error as the
# y-variable.

# Use geom_boxplot to indicate that we want to plot a boxplot.

# Use geom_point to add data points as a layer.

# The `errors` data have already been loaded. Examine them using the head function.
head(errors)

# Create a boxplot showing the errors by state for polls with grades B+ or higher

errors %>%
  filter(grade %in% c("A+", "A", "A-", "B+")) %>%
  ggplot(aes(state, error)) +
  geom_boxplot() +
  geom_point()

# Exercise 10. Filter error plot

# Some of these states only have a few polls. Repeat the previous exercise to plot
# the errors for each state, but only include states with five good polls or more.

# Use the filter function to filter the data for polls with grades equal to A+, A, 
# A-, or B+.

# Group the filtered data by state using group_by

# Use the filter function to filter the data for states with at least 5 polls. Then,
# use ungroup so that polls are no longer grouped by state.

# Use the reorder function to order the state data by error.

# Using ggplot, set the aesthetic with state as the x-variable and error as the y-variable.

# Use geom_boxplot to indicate that we want to plot a boxplot.

# Use geom_point to add data points as a layer.

# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Create a boxplot showing the errors by state for states wth at least 5 polls with
# grades B+ or higher

errors %>% 
  filter(grade %in% c("A+", "A", "A-", "B+")) %>%
  group_by(state) %>%
  filter(n() >= 5) %>%
  ungroup() %>%
  mutate(state = reorder(state, error, FUN = median)) %>%
  ggplot(aes(state, error)) +
  geom_boxplot() +
  geom_point()

# THE T-DISTRIBUTION

# In models where we must estimate two parameters, p and `sigma`, the Central Limit
# Theorem can result in overconfident confidence intervals for sample sizes smaller
# than approximately 30.

# If the population data are known to follow a normal distribution, theory tells us
# how much larger to make confidence intervals to account for estimation of `sigma`

# Given s as an estimate of `sigma`, then Z = X-bar - d / s / sqrt(N) follows a 
# t-distribution with N - 1 degrees of freedom.

# Degrees of freedom determine the weight of the tails of the distribution. Small
# values of degrees of freedom lead to increased probabilities of extreme values.

# We can determine confidence intervals using the t-distribution instead of the 
# normal distribution by calculating the desired quantile with the function qt()

# Calculating 95% confidence intervals with the t-distribution
z <- qt(0.975, nrow(one_poll_per_pollster) - 1)
one_poll_per_pollster %>%
  summarize(avg = mean(spread), moe = z * sd(spread) / sqrt(length(spread))) %>%
  mutate(start = avg - moe, end = avg + moe)

# quantile from t-distribution versus normal distribution
qt(0.975, 14) # 14 = nrow(one_poll_per_polster)
qnorm(0.975)

# ASSESSMENT 6.2. THE T-DISTRIBUTION

# Exercise 1. Using the t-distribution

# We know that, with a normal distribution, only 5% of values are more than 2
# standard deviations away from the mean.

# Calculate the probability of seeing t-distributed random variables being more
# than 2 in absolute value when the degrees of freedom are 3.

# Use the pt function to calculate the probability of seeing a value less than or
# equal to the argument. Your output should be a single value.

# Calculate the probability of seeing t-distributed random variables being more than
# 2 in absolute value when `df = 3`

1 - pt(2, 3) + pt(-2, 3)

# Exercise 2. Plotting the t-distribution

# Now use sapply to compute the same probability for degrees of freedom from 3 to 50.

# Make a plot and notice when this probability converges to the normal distribution's 5%.

# Make a vector called df that contains a sequence of numbers from 3 to 50.

# Using function, make a function called pt_func that recreates the calculation for the
# probability that a value is greater than 2 as an absolute value for any given degrees
# of freedom.

# Use sapply to apply the pt_func function across all values contained in df. Call these
# probabilities probs.

# Use the plot function to plot df on the x-axis and probs on the y-axis.

# Generate a vector `df` that contains a sequence of numbers from 3 to 50

df <- seq(3, 50)

# Makea a function called `pt_func` that calculates the probability that a value is more
# than |2| for any degrees of freedom

pt_func <- function(x){
  1 - pt(2, x) + pt(-2, x)
}

# Generate a vector `probs` that uses the `pt_func` function to calculate the probabilities

probs <- sapply(df, pt_func)

# Plot `df` on the x-axis and `probs` on the y-axis

plot(df, probs)

# Exercise 3. Sampling from the normal distribution

# In a previous section, we repeatedly took random samples of 50 heights from a distribution
# of heights. We noticed that about 95% of the samples had confidence intervals spanning
# the true population mean.

# Re-do this Monte Carlo simulation, but now instead of N = 50, use N = 15. Notice what
# happens to the proportion of hits.

# Use the replicate function to carry out the simulation. Specify the number of times
# you want the code to run and, within brackets, the three lines of code that should run.

# First use the sample function to randomly sample N values of x.

# Second create a vector called interval that calculates the 95% confidence interval for
# the sample. You will use the qnorm function.

# Third, use the between function to determine if the population mean mu is contained
# between the confidence intervals.

# Save the results of the Monte Carlo function to a vector called `res`.

# Use the mean function to determine the proportion of hits in `res`.

# Load the necessary libraries and data
library(dslabs)
library(dplyr)
data(heights)

# Use the sample code to generate `x`, a vector of male heights
x <- heights %>% filter(sex == "Male") %>% .$height

# Create variables for the mean height `mu`, the sample size `N`, and the number
# of times the simulation should run `B`
mu <- mean(x)
N <- 15
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result
# after random sampling
set.seed(1, sample.kind = "Rounding")

# Generate a logical vector `res` that contains the results of the simulations

res <- replicate(B, {
  X <- sample(x, N, replace = TRUE)
  interval <- c(mean(X) - qnorm(.975) * sd(X) / sqrt(N),
                mean(X) + qnorm(.975) * sd(X) / sqrt(N))
  between(mu, interval[1], interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95% 
# confidence interval. Print this value to the console.

mean(res)

# Exercise 4. Sampling from the t-distribution

# N = 15 is not that big. We know that heights are normally distributed, so the
# t-distribution should apply. Repeat the previous Monte Carlo simulation using the
# t-distribution instead of using the normal distribution to construct the confidence
# intervals.

# What are the proportion of 95% confidence intervals that span the actual mean height now?

# Use the replicate function to carry out the simulation. Specify the number of times you
# want the code to run and, within brackets, the three lines of code that should run.

# First use the sample function to randomly sample N values from x.

# Second, create a vector called interval that calculates the 95% confidence interval for
# the sample. Remember to use the qt function this time to generate the confidence
# interval.

# Third, use the between function to determine if the population mean `mu` is contained
# between the confidence intervals.

# Save the results of the Monte Carlo function to a vector called `res`.

# Use the mean function to determine the proportion of hits in `res`.

# The vector of filtered heights `x` has already been loaded for you. Calculate the mean.
mu <- mean(x)

# Use the same sample parameters as in the previous exercise.
set.seed(1, sample.kind = "Rounding")
N <- 15
B <- 10000

# Generate a logical vector `res` that contains the results of the simulations using
# the t-distribution

res <- replicate(B, {
  X <- sample(x, N, replace = TRUE)
  interval <- c(mean(X) - qt(0.975, N - 1) * sd(X) / sqrt(N),
                mean(X) + qt(0.975, N - 1) * sd(X) / sqrt(N))
  between(mu, interval[1], interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95%
# confidence interval. Print this value to the console.

mean(res)

# Exercise 5. Why the t-distribution

# Why did the t-distribution confidence intervals work so much better?

# The t-distribution takes the variability into account and generates larger
# confidence intervals # Correct

# Because the t-distribution shifts the intervals in the direction towards the
# actual mean

# This was just a chance occurrence. If we run it again, the CLT will work better.

# The t-distribution is always a better approximation than the normal distribution.

## SECTION 7. ASSOCIATION TESTS

# You will be able to:

# Use association and chi-squared tests to performa inference on binary, categorical,
# and ordinal data.

# Calculate an odds ratio to get an idea of the magnitude of an observed effect.

# ASSOCIATION TESTS

# We learn how to determine the probability that an observation is due to random
# variability given categorical, binary, or ordinal data.

# Fisher's exact test determines the p-value as the probability of observing an outcome
# as extreme or more extreme than the observed outcome given the null distribution.

# Data from a binary experiment are often summarized in two-by-two tables.

# The p-value can be calculated from a two-by-two table using Fisher's exact test with
# the function fisher.test.

# Research funding rates example

# load and inspect the research funding rates object
library(tidyverse)
library(dslabs)
data(research_funding_rates)
research_funding_rates

# compute totals that were successful or not successful
totals <- research_funding_rates %>%
  select(-discipline) %>%
  summarize_all(funs(sum)) %>%
  summarize(yes_men = awards_men,
            no_men = applications_men - awards_men,
            yes_women = awards_women,
            no_women = applications_women - awards_women)

# compare percentage of men/women with awards
totals %>% summarize(percent_men = yes_men / (yes_men + no_men),
                     percent_women = yes_women / (yes_women + no_women))

# Two-by-two table and p-value for the Lady Tasting Tea problem
tab <- matrix(c(3, 1, 1, 3), 2, 2)
rownames(tab) <- c("Poured Before", "Poured After")
colnames(tab) <- c("Guessed Before", "Guessed After")
tab

# p-value calculation with Fisher's Exact Test
fisher.test(tab, alternative = "greater")

# CHI-SQUARED TESTS

# If the sums of the rows and the sums of the columns in the two-by-two table are fixed,
# then the hypergeometric distribution and Fisher's exact test can be used. Otherwise,
# we must use the chi-squared test.

# The chi-squared test compares the observed two-by-two table to the two-by-two table
# expected by the null hypothesis and asks how likely it is that we see a deviation as
# large as observed or larger by chance.

# The function chisq.test() takes a two-by-two table and returns the p-value from the
# chi-squared test.

# The odds ratio states how many times larger the odds of an outcome are for one group
# relative to another group.

# A small p-value does not imply a large odds ratio. If a finding has a small p-value but
# also a small odds ratio, it may not be a practically significant or scientifically
# significant finding.

# Because the odds ratio is a ratio of ratios, there is no simple way to use the Central
# Limit Theorem to compute confidence intervals. There are advanced methods for computing
# confidence intervals for odds ratios that we do not discuss here.

# Chi-squared test

# compute overall funding rate
funding_rate <- totals %>%
  summarize(percent_total = (yes_men + yes_women) / (yes_men + yes_women + no_men +
                                                       no_women)) %>%
  .$percent_total
funding_rate

# construct a two-by-two table for observed data
two_by_two <- tibble(awarded = c("no", "yes)"),
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two

# compute null hypothesis two-by-two table
tibble(awarded = c("no", "yes"),
       men = (totals$no_men + totals$yes_men) * c(1 - funding_rate, funding_rate),
       women = (totals$no_women + totals$yes_women) * c(1 - funding_rate, funding_rate))

# chi-squared test
chisq_test <- two_by_two %>%
  select(-awarded) %>%
  chisq.test()
chisq_test$p.value

# Odds ratio

# odds of getting funding for men
odds_men <- (two_by_two$men[2] / sum(two_by_two$men)) /
  (two_by_two$men[1] / sum(two_by_two$men))

# odds of getting funding for women
odds_women <- (two_by_two$women[2] / sum(two_by_two$women)) /
  (two_by_two$women[1] / sum(two_by_two$women))

# odds ration - how many times larger odds are for men than women
odds_men / odds_women

# p-value and odds ratio responses to increasing sample size
two_by_two %>%
  select(-awarded) %>%
  mutate(men = men * 10, women = women * 10) %>%
  chisq.test()

# ASSESSMENT 7.1. ASSOCIATION AND CHI-SQUARED TESTS

# Exercise 1. Comparing proportions of hits

# In a previous exercise, we determined whether or not each poll predicted the correct
# winner for their state in the 2016 U.S. presidential election. Each poll was also
# assigned a grade by the poll aggregator. Now we're going to determine if polls rated
# A- made better predictions than polls rated C-.

# In this exercise, filter the errors data for just polls with grades A- and C-. Calculate
# the proportion of times each grade of poll predicted the correct winner.

# Filter `errors` for grades A- and C-.

# Group the data by grade and hit.

# Summarize the number of hits for each grade.

# Generate a two-by-two table containing the number of hits and misses for each grade.
# Try using the spread function to generate this table.

# Calculate the proportion of times each grade was correct.

# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Generate an object called `totals` that contains the numbers of good and bad predictions
# for polls rated A- and C-.

library(tidyverse)

totals <- errors %>%
  filter(grade %in% c("A-", "C-")) %>%
  group_by(grade, hit) %>%
  summarize(n = n()) %>%
  spread(grade, n)

# Print the proportion of hits for grade A- polls to the console

totals$`A-`[2] / sum(totals$`A-`)

# Print the proportion of hits for grade C- polls to the console

totals$`C-`[2] / sum(totals$`C-`)

# Exercise 2. Chi-squared test

# We found that the A- polls predicted the correct winner about 80% of the time in their
# states and C- polls predicted the correct winner about 86% of the time.

# Use a chi-squared test to determine if these proportions are different.

# Use the chisq.test function to perform the chi-squared test. Save the results to an
# object called chisq_test.

# Print the p.value of the test to the console.

# The `totals` data have already been loaded. Examine them using the `head` function.
head(totals)

# Perform a chi-squared test on the hit data. Save the results as an object called 
# `chisq_test`

chisq_test <- totals %>%
  select(-hit) %>%
  chisq.test()

# Print the p-value of the chi-squared test to the console

chisq_test$p.value

# Exercise 3. Odds ratio calculation

# It doesn't look like the grade A- polls performed significantly differently than the
# grade C- polls in their states.

# Calculate the odds ration to determine the magnitude of the difference in performance
# between these two grades of polls.

# Calculate the odds that a grade C- poll predicts the correct winner. Save this result to
# a variable called `odds_C`.

# Calculate the odds that a grade A- poll predicts the correct winner. Save this result to
# a variable called `odds_A`.

# Calculate the odds ration that tells us how many times larger the odds of a grade A- poll
# is at predicting the winner than a grade C- poll.

# The `totals` data have already been loaded. Examine them using the `head` function
head(totals)

# Generate a variable called `odds_C` that contains the odds of getting the prediction
# right for grade C- polls

odds_C <- (totals$`C-`[2] / sum(totals$`C-`)) / (totals$`C-`[1] / sum(totals$`C-`))

# Generate a variable called `odds_A` that contains the odds of getting the prediction
# right for grade A- polls

odds_A <- (totals$`A-`[2] / sum(totals$`A-`)) / (totals$`A-`[1] / sum(totals$`A-`))

# Calculate the odds ratio to determine how many times larger the odds ration is for
# grade A- polls than grade C- polls

odds_A / odds_C

# Exercise 4. Significance

# We did not find meaningful differences between the poll results from grade A- and 
# grade C- polls in this subset of the data, which only contains polls for about a 
# week before the election. Imagine we expanded our analysis to include all election polls
# and we repeat our analysis. In this hypothetical scenario, we get that the p-value for 
# the difference in prediction success if 0.0015 and the odds ration describing the effect
# size of the performance of grade A- over grade B- polls is 1.07.

# Based on what we learned in the last section, which statement reflects the best
# interpretation of this result?

# The p-value is below 0.05, so there is a significant difference. Grade A- polls are
# significantly better at predicting winners.

# The p-value is too close to 0.05 to call this a significant difference. We do not
# observe a difference in performance.

# The p-value is below 0.05, but the odds ratio is very close to 1. There is not a
# scientifically significant difference in performance. # Correct

# The p-value is below 0.05 and the odds ratio indicates that grade A- polls perform
# significantly better than grade C- polls.

## COURSE WRAP-UP AND COMPREHENSIVE ASSESSMENT: BREXIT

# In June 2016, the United Kingdom (UK) held a referendum to determine whether the country
# would "Remain" in the European Union (EU) or "Leave" the EU. This referendum is commonly
# known as Brexit. Although the media and others interpreted poll results as forecasting
# "Remain" (p > 0.5), the actual proportion that voted "Remain" was only 48.1% (p = 0.481)
# and the UK thus voted to leave the EU. Pollsters in the UK were criticized for
# overestimating support for "Remain".

# In this project, you will analyze real Brexit polling data to develop polling models
# to forecast Brexit results. You will write your own code in R and enter the answers.

# Important definitions

# Data Import: Import the brexit_polls data from the dslabs package and set options for
# the analysis:

# suggested libraries and options
library(tidyverse)
options(digits = 3)

# load brexit_polls object
library(dslabs)
data(brexit_polls)

# Final Brexit parameters

# Define p = 0.481 as the actual percent voting "Remain" on the Brexit referendum and
# d = 2p - 1 = -0.038 as the actual spread of the Brexit referendum with "Remain" defined
# as the positive outcome.

p <- 0.481 # official proportion voting "Remain"
d <- 2 * p - 1 # official spread

# Question 1. Expected value and standard error of a poll

# The final proportion of voters choosing "Remain" was p = 0.481. Consider a poll with a
# sample of N = 1,500 voters.

# What is the expected total number of voters in the sample choosing "Remain"?

1500 * p

# What is the standard error of the total number of voters in the sample choosing "Remain"?

1500 * sqrt(p*(1-p)/1500)

# What is the expected value of X-hat, the proportion of "Remain" voters?

p

# What is the standard error of X-hat, the proportion of "Remain" voters?

sqrt(p * (1 - p) / 1500)

# What is the expected value of d, the spread between the proportion of "Remain" voters
# and "Leave" voters?

2 * p - 1

# What is the standard error of d, the spread between the proportion of "Remain" voters
# and "Leave" voters?

2 * sqrt(p * (1 - p) / 1500)

# Question 2. Actual Brexit poll estimates

# Load and inspect the brexit_polls dataset from dslabs, which contains actual polling data
# for the 6 months before the Brexit vote. Raw proportions of voters preferring "Remain",
# "Leave", and "Undecided" are available (`remain`, `leave`, `undecided`). The spread is
# also available (`spread`), which is the difference in the raw proportion of voters
# choosing "Remain" and the raw proportion choosing "Leave".

# Calculate `x_hat` for each poll, the estimate of the proportion of voters choosing "Remain"
# on the referendum day (p= 0.481), given the observed value spread and the relationship
# d-hat = 2 * x_hat - 1. Use mutate() to add a variable x_hat to the brexit_polls object
# by filling in the skeleton code below:

# brexit_polls <- brexit_polls %>%
#  mutate(x_hat = ?)

head(brexit_polls)
brexit_polls <- brexit_polls %>%
  mutate(x_hat = (spread + 1) / 2)

# What is the average of the observed spreads (`spread`)?

mean(brexit_polls$spread)

# What is the standard deviation of the observed spreads?

sd(brexit_polls$spread)

# What is the average of x_hat, the estimates of parameter p?

mean(brexit_polls$x_hat)

# What is the standard deviation of x_hat?

sd(brexit_polls$x_hat)

# Question 3. Confidence interval of a Brexit poll

# Consider the first poll in brexit_polls, a YouGov poll run on the same day as the
# Brexit referendum:

brexit_polls[1,]

# Use qnorm() to compute the 95% confidence interval for X-hat.

# What is the lower bound of the 95% confidence interval?

brexit_polls$x_hat[1] - qnorm(0.975) * sqrt(brexit_polls$x_hat[1] * 
                                              (1 - brexit_polls$x_hat[1]) 
                                            / brexit_polls$samplesize[1])

# What is the upper bound of the 95% confidence interval?

brexit_polls$x_hat[1] + qnorm(0.975) * sqrt(brexit_polls$x_hat[1] * 
                                              (1 - brexit_polls$x_hat[1]) 
                                            / brexit_polls$samplesize[1])

# Does the 95% confidence interval predict a winner (does not cover p = 0.5)? Does
# the 95% confidence interval cover the true value of p observed during the referendum?

# The interval predicts a winner and covers the true value of p.
# The interval predicts a winner but does not cover the true value of p. # Correct
# The interval does not predict a winner but does cover the true value of p.
# The interval does not predict a winner and does not cover the true value of p.

# BREXIT POLL ANALYISIS - PART 2

# Question 4. Create the data frame `june_polls` containing only Brexit polls in June
# 2016 (`enddate` of "2016-06-01" and later). We will calculate confidence intervals for
# all polls and determine how many cover the true value of d.

# First, use `mutate()` to calculate a plug-in estimate se_x_hat for the standard error
# of the estimate SE-hat[X] for each poll given its sample size and value of X-hat. Second,
# use mutate() to calculate an estimate for the standard error of the spread for each poll
# given the value of se_x_hat. Then, use mutate() to calculate the upper and lower bounds
# for 95% confidence intervals of the spread. Last, add a column `hit` that indicates
# whether the confidence interval for each poll covers the correct spread d = -0.038.

# How many polls are in `june_polls`?

june_polls <- brexit_polls %>%
  filter(enddate >= "2016-06-01")
nrow(june_polls)

june_polls <- june_polls %>%
  mutate(se_x_hat = sqrt(x_hat * (1 - x_hat) / samplesize),
         se_spread = 2 * se_x_hat,
         lower = spread - qnorm(.975) * se_spread,
         upper = spread + qnorm(.975) * se_spread,
         hit = lower <= d & upper >= d)

# What proportion of polls have a confidene interval that covers the value 0?

mean(june_polls$lower <= 0 & june_polls$upper >= 0)

# What proportion of polls predict "Remain" (confidence interval entirely above 0)?

mean(june_polls$lower > 0)

# What proportion of polls have a confidence interval covering the true value of d?

mean(june_polls$hit)

# Question 5. Hit rate by pollster

# Group and summarize the `june_polls` object by pollster to find the proportion of
# hits for each pollster and the number of polls per pollster. Use arrange() to sort
# by hit rate.

june_polls %>%
  group_by(pollster) %>%
  summarize(n = n(), rate = mean(hit)) %>%
  arrange(rate)

# Which of the following are true?

# Unbiased polls and pollster wills theoretically cover the correct value of the spread
# 50% of the time

# Only one pollster had a 100% success rate in generating confidence intervals that
# covered the correct value of the spread.

# The pollster with the highest number of polls covered the correct value of the spread
# in their confidence interval over 60% of the time.

# All pollsters produced confidence intervals covering the correct spread in at least
# 1 of their polls.

# The results are consistent with a large general bias that affects all pollsters. # Correct

# Question 6. Boxplot of Brexit polls by poll type

# Make a boxplot of the spread in `june_polls` by poll type.

june_polls %>% ggplot(aes(poll_type, spread)) +
  geom_boxplot()

# Which of the following are true?

# Online polls tend to show support for "Remain" (`spread` > 0)

# Telephone polls tend to show support for "Remaain" (`spread` > 0) # Correct

# Telephone polls tend to show higher support for "Remain" than online polls
# (higher `spread`) # Correct

# Online polls have a larger interquartile range (IQR) for the spread than telephone
# polls, indicating that they are more variable. # Correct

# Poll type introduces a bias that affects poll results. # Correct

# Question 7. Combined spread across poll type

# Calculate the confidence intervals of the spread combined across all polls in
# `june_polls`, grouping by poll type. Recall that to determine the standard error
# of the spread, you will need to double the standard error of the estimate.

# Use this code (which determines the total sample size per poll type, gives each
# spread estimate a weight based on the poll's sample size, and adds the estimate of
# p from the combined spread) to being your analysis.

combined_by_type <- june_polls %>%
  group_by(poll_type) %>%
  summarize(N = sum(samplesize),
            spread = sum(spread * samplesize / N),
            p_hat = (spread + 1) / 2)

# What is the lower bound of the 95% confidence interval for online voters?

combined_by_type$spread[1] - qnorm(.975) * 2 * 
  (sqrt(combined_by_type$p_hat[1] * (1 - combined_by_type$p_hat[1]) / combined_by_type$N[1]))

# What is the upper bound of the 95% confidence interval for online voters?

combined_by_type$spread[1] + qnorm(.975) * 2 * 
  (sqrt(combined_by_type$p_hat[1] * (1 - combined_by_type$p_hat[1]) / combined_by_type$N[1]))

# Correct, but saved intervals like this:

combined_by_type <- june_polls %>%
  group_by(poll_type) %>%
  summarize(N = sum(samplesize),
            spread = sum(spread * samplesize / N),
            p_hat = (spread + 1) / 2,
            se_spread = 2 * sqrt(p_hat * (1 - p_hat) / N),
            spread_lower = spread - qnorm(.975) * se_spread,
            spread_upper = spread + qnorm(.975) * se_spread)

# Question 8. Interpreting combined spread estimates across poll type

# Interpret the confidence intervals for the combined spreads for each poll type calculated
# in the previous problem.

# Which of the following are TRUE about the confidence intervals of the combined spreads
# for different poll types?

# Neither set of combined polls makes a prediction about the outcome of the Brexit referendum
# (a prediction is possible if a confidence interval does not cover 0) # Correct

# The confidence interval for online polls is larger than the confidence interval for
# telephone polls.

# The confidence interval for telephone polls covers more positive values than the confidence
# interval for online polls. # Correct

# The confidence intervals for different poll types do not overlap.

# Neither confidence interval covers the true value of d = -0.038. # Correct

# BREXIT POLL ANALYSIS - PART 3

# Question 9. Chi-squared p-value

# Define `brexit_hit`, with the following code, which computes the confidence intervals
# for all Brexit polls in 2016 and then calculates whether the confidence interval covers
# the actual value of the spread d = -0.038:

brexit_hit <- brexit_polls %>%
  mutate(p_hat = (spread + 1) / 2,
         se_spread = 2 * sqrt(p_hat * (1 - p_hat) / samplesize),
         spread_lower = spread - qnorm(.975) * se_spread,
         spread_upper = spread + qnorm(.975) * se_spread,
         hit = spread_lower < -0.038 & spread_upper > -0.038) %>%
  select(poll_type, hit)

# Use `brexit_hit` to make a two-by-two table of poll type and hit status. Then use
# chisq.test() to perform a chi-squared test to determine whether the difference in
# hit rate is significant.

brexit_hit %>%
  group_by(poll_type, hit) %>%
  summarize(n = n()) %>%
  spread(poll_type, n) %>%
  chisq.test()

# What is the p-value of the chi-squared test comparing the hit rate of online and
# telephone polls?

# Determine which poll type has a higher probability of producing a confidence interval
# that covers the correct value of the spread. Also determine whether this difference is
# staistically significant at a p-value cutoff of 0.05. Which of the following is true?

# Online polls are more likely to cover the correct value of the spread and this
# difference is statistically significant. # Correct

# Online polls are more likely to cover the correct value of the spread, but this difference
# is not statistically significant.

# Telephone polls are more likely to cover the correct value of the spread and this
# difference is statistically significant.

# Telephone polls are more likely to cover the correct value of the spread, but this
# difference is not statistically significant.

# Question 10. Odds ratio of online and telephone poll hit rate.

# Use the two-by-two table constructed in the previous exercise to calculate the odds
# ratio between the hit rate of online and telephone polls to determine the magnitude
# of the difference in performance between the poll types.

# Calculate the odds that an online poll generates a confidence interval that covers
# the actual value of the spread.

two_by_two <- brexit_hit %>%
  group_by(poll_type, hit) %>%
  summarize(n = n()) %>%
  spread(poll_type, n)

odds_online <- (two_by_two$Online[2] / sum(two_by_two$Online)) /
  (two_by_two$Online[1] / sum(two_by_two$Online))

# Calculate the odds that a telephone poll generates a confidence interval that covers
# the actual value of the spread.

odds_telephone <- (two_by_two$Telephone[2] / sum(two_by_two$Telephone)) /
  (two_by_two$Telephone[1] / sum(two_by_two$Telephone))

# Calculate the odds ration to determine how many times larger the odds are for online
# polls to hit versus telephone polls

odds_online / odds_telephone

# Question 11. Plotting spread over time

# Use `brexit_polls` to make a plot of the spread (`spread`) over time (`enddate`) colored
# by poll type (`poll_type`). Use geom_smooth() with method = "loess" to plot smooth
# curves with a span of 0.4. Include the individual data points colored by poll type. Add
# a horizontal line indicating the final value of d = -0.038.

# Which of the following plots is correct?

brexit_polls %>%
  ggplot(aes(enddate, spread, color = poll_type)) +
  geom_smooth(method = "loess", span = 0.4) +
  geom_point() +
  geom_hline(yintercept = -0.038)

# Question 12. Plotting raw percentages over time

# Use the following code to create the object `brexit_long`, which has a column `vote`
# containing the three possible votes on a Brexit poll ("remain", "leave", "undecided")
# and a column `porportion` containing the raw proportion choosing that vote option on
# the given poll:

brexit_long <- brexit_polls %>%
  gather(vote, proportion, "remain":"undecided") %>%
  mutate(vote = factor(vote))

# Make a graph of proportion over time colored by vote. Add a smooth trendline with
# geom_smooth() and method = "loess" with a span of 0.3

brexit_long %>%
  ggplot(aes(enddate, proportion, color = vote)) +
  geom_smooth(method = "loess", span = 0.3)

# Which of the following are TRUE?

# The percentage of undecided voters declines over time but is still around 10%
# throughout June. # Correct

# Over most of the date range, the confidence bands for "Leave" and "Remain" 
# overlap. # Correct.

# Over most of the date range, the confidence bands for "Leave" and "Remain" are
# below 50%. # Correct

# In the first half of June, "Leave" was polling higher than "Remain", although
# this difference was within the confidence intervals. # Correct

# At the time of the election in late June, the percentage voting "Leave" is
# trending upwards.